{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27876baf936b6237",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction\n",
    "In this era of rapidly advancing digital technologies, the intersection of privacy and technological innovation has become a focal point of concern. Our project, \"Incognito Face,\" conceived and developed as part of the Medienverarbeitung 2023/2024 curriculum, seeks to address these concerns by developing a solution that can effectively disrupt these technologies. This introduction outlines the objective, approach, and structure of our project, emphasizing the significance of our goal and the distinct division of our work into two primary components: Face Detection (FD) and Face Recognition (FR).\n",
    "\n",
    "#### Project Objective\n",
    "Our primary objective is to implement undetectable filters that significantly hinder state-of-the-art face detection and recognition technologies. We aim to achieve this by increasing the false-positive rates of these technologies without altering the visual perception of the human eye. In essence, our goal is to enhance online privacy by reducing the accuracy of automated facial recognition systems, particularly in scenarios where individuals share images on digital platforms.\n",
    "\n",
    "#### Approach and Methodology\n",
    "Our approach involves a user-centric design where individuals can upload custom photos and apply a selection of our implemented filters. These filters are designed to test their efficiency in disrupting face detection and recognition technologies. We have incorporated a range of filters, including those that prevent face detection, increase false positives, and artistic filters, all developed with the intention of preserving the original image's aesthetic while impairing automated recognition systems.\n",
    "\n",
    "#### Project Structure\n",
    "**Face Detection (FD)**\n",
    "\n",
    "The FD component of our project focuses on developing filters that prevent state-of-the-art face detection algorithms from detecting faces. This involves a dive into understanding and experimenting with various algorithms like Viola Jones, Histogram of Oriented Gradients (HOG) + Support Vector Machine (SVM), MTCNN, SSD, and CNN. Our efforts here are geared towards artistic modifications to pictures that can fool these detection algorithms or to create the illusion of more faces to cause false-positives.\n",
    "\n",
    "**Face Recognition (FR)**\n",
    "\n",
    "In the FR segment, we extend our work to challenge face recognition systems. This part of the project is dedicated to implementing filters that can effectively mask or morph key facial features in a manner that causes recognition algorithms to fail or misidentify subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2f7327b09d658",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Face Detection\n",
    "\n",
    "In this section, the state-of-the-art face detection algorithms will be introduced. Afterwards, the filters that prevent the algorithms from detecting faces, the filters that increase the false positive rate of face detection, and artitstic filters will be investigated.\n",
    "\n",
    "## Face Detection Algorithms\n",
    "\n",
    "### Viola Jones\n",
    "\n",
    "The Viola-Jones face detection algorithm is a widely used and efficient method for detecting faces in images.\n",
    "It was proposed by Paul Viola and Michael Jones in their 2001 paper, \"Rapid Object Detection using a Boosted Cascade of Simple Features.\"\n",
    "\n",
    "The Viola-Jones algorithm employs a machine learning approach, specifically a variant of the AdaBoost algorithm, to train a cascade of classifiers for face detection. The training process involves selecting a set of Haar-like features, which are simple rectangular patterns that can be computed quickly. These features capture local intensity variations in the image.\n",
    "\n",
    "In the following, we will give a brief overview of the steps in Viola-Jones.\n",
    "\n",
    "#### Step 1: Selecting Haar-like features\n",
    "\n",
    "Haar-like features are essential building blocks in the Viola-Jones face detection algorithm,\n",
    "capturing distinctive patterns in faces. These features are rectangular and can take various forms,\n",
    "such as edges, lines, or rectangles with different orientations.\n",
    "\n",
    "For example, a Haar-like feature might capture the contrast between the eyes and the nose. The choice\n",
    "of these features is crucial as they serve as the basis for distinguishing between positive (faces) and\n",
    "negative (non-faces) examples during the training phase.\n",
    "\n",
    "Here's a simple example image illustrating a Haar-like feature capturing the vertical contrast\n",
    "between the left and right sides of a face:\n",
    "\n",
    "<div>\n",
    "<img src=\"images/haar-like-features.png\" width=\"400\"/>\n",
    "</div>\n",
    "#### Step 2 - Creating an integral image\n",
    "\n",
    "To efficiently compute Haar-like features, the Viola-Jones algorithm uses an integral image. The integral\n",
    "image is a transformed version of the original image, where each pixel represents the cumulative sum of\n",
    "all pixels above and to the left of it.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/integral-image.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "The integral image enables rapid calculation of the sum of pixel values within any rectangular region,\n",
    "which is essential for evaluating Haar-like features in constant time.\n",
    "\n",
    "#### Step 3 - Running AdaBoost training\n",
    "\n",
    "AdaBoost is a machine learning algorithm employed by the Viola-Jones face detection method to create\n",
    "a robust and accurate classifier. In this context, the weak classifiers are decision stumps based on\n",
    "Haar-like features.\n",
    "\n",
    "The AdaBoost training process involves iteratively selecting the best weak classifiers while assigning\n",
    "higher weights to misclassified examples from the previous iteration. This iterative process continues\n",
    "until a predefined number of weak classifiers are trained.\n",
    "\n",
    "Consider an example image dataset with positive examples (faces) and negative examples (non-faces).\n",
    "During AdaBoost training, the algorithm learns to focus on the features that effectively discriminate\n",
    "between the two classes, building a strong classifier that is adept at face detection.\n",
    "\n",
    "#### Step 4 - Creating classifier cascades\n",
    "\n",
    "The trained AdaBoost classifier is organized into a cascade of stages in the Viola-Jones algorithm.\n",
    "Each stage consists of multiple weak classifiers applied sequentially. The cascade structure allows\n",
    "for the rapid rejection of non-face regions, contributing to the algorithm's efficiency.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/cascade-classifier.png\" width=\"900\"/>\n",
    "</div>\n",
    "\n",
    "The cascade of classifiers is constructed in such a way that a region of the image must pass all\n",
    "the classifiers in a stage to be considered a potential face region. If at any stage a region fails\n",
    "to pass a classifier, it is promptly rejected, saving computational resources. This cascade structure\n",
    "enhances the Viola-Jones algorithm's speed, making it well-suited for real-time face detection applications.\n",
    "\n",
    "#### Practical Applications\n",
    "Our python implementation for Viola-Jones is using the following Code in the backend:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87d96827eaa69833",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-14T09:31:03.896310200Z",
     "start_time": "2024-02-14T09:31:03.788763300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhighlight_face_viola_jones\u001b[39m(img: \u001b[43mImage\u001b[49m):\n\u001b[0;32m      2\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(numpy\u001b[38;5;241m.\u001b[39marray(img), cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR)\n\u001b[0;32m      3\u001b[0m     gray_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "def highlight_face_viola_jones(img: Image):\n",
    "    img = cv2.cvtColor(numpy.array(img), cv2.COLOR_RGB2BGR)\n",
    "    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = viola_jones_detector.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(40, 40))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 6)\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(img_rgb), len(faces), '?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd021e2e287d8fd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This implementation of Viola-Jones face detection processes a given PIL Image object. It converts the image to BGR format, then to grayscale. Using a pre-trained Haar cascade classifier for frontal faces, it detects faces in the grayscale image. This pre-trained classifier is already provided by the library, so we do not need to train or create our own. Detected faces are outlined with blue rectangles, and the modified image is converted back to RGB format before being returned. The algorithm provides a visual representation of the input image with highlighted face regions.\n",
    "\n",
    "Here is an example output of the algorithm:\n",
    "\n",
    "<div>\n",
    "<img src=\"images/detected-faces-examples/detected_face_viola_jones.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518e3eacba0f0015",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### HOG and SVM\n",
    "Another approach to detect faces is using a Histogram of Oriented Gradients (HOG) in combination with a support vector machine as classifier.\n",
    "HOG is a feature descriptor and is commonly used in image processing that was published by Dalal and Triggs. The algorithm typically consists of the following steps:\n",
    "1. Image Preprocessing\n",
    "2. Calculate Gradient\n",
    "3. Create Histogram of Oriented Gradients\n",
    "4. Normalise Histogram Vectors\n",
    "\n",
    "In the following, we will give a brief overview of the steps in HOG.\n",
    "\n",
    "#### Step 1 - Image Preprocessing\n",
    "\n",
    "HOG (Histogram of Oriented Gradients) is only used on the part of the image that is relevant for examining a particular subject (a face in our case). For this purpose, it is necessary to first crop this part of the image. For the subsequent calculation of gradients in blocks, the cropped image is resized to a width-to-height ratio of 1:2. In the publication by Dalal and Triggs, 64:128 was chosen because it provided enough information for pedestrian recognition, which was the primary focus of the publication. This image is then divided into blocks of size 8x8, as features are extracted from blocks of pixels rather than individual pixels. Graphically, one can envision this as an 8x16 grid of 8x8 blocks drawn on the image.\n",
    "\n",
    "#### Step 2 - Calculate Gradient\n",
    "\n",
    "Since edges represent the boundaries between regions in the image with a significant change in intensity, they are essential to determine the contours of an image. The contours of an image often suffice to classify the objects in the image (a face in our case). To determine the edges, the gradient vector is used because it indicates the direction of the greatest local change in intensity, and its magnitude represents the extent of the change. The gradient vector of a 2-dimensional image is mathematically the partial derivative in the x and y directions. Since the colors of images in computers are represented by discrete color values and are not continuous as in reality, the change in the x and y directions is calculated as follows:\n",
    "Let $I$ be a function that takes as input the x and y positions of a pixel in the image and outputs the intensity (between 0 and 255). Then, the partial derivative in the x-direction and hence the gradient component $G_x$ is calculated as follows: $G_x = I(x+1, y) - I(x-1, y)$. Similarly, the partial derivative in the y-direction and hence the gradient component $G_y$ is calculated as follows: $G_y = I(x, y+1) - I(x, y-1)$. Thus, the changes in intensity are calculated by considering the horizontal and vertical neighbors of a pixel.\n",
    "Consider the following image as an example for the pixel with intensity 60, where only 4 out of the 64 values of the 8x8 block are displayed:\n",
    "\n",
    "![Alt text](images/Hog8x8Grid.png)\n",
    "\n",
    "For the pixel 60 on the image, the gradient in x-direction will be:\n",
    "$$G_x = I(x+1, y) - I(x-1, y) = 70 - 40 = 30$$\n",
    "\n",
    "and for the y-direction:\n",
    "$$G_y = I(x, y+1) - I(x, y-1) = 70 - 20 = 50$$\n",
    "\n",
    "Using the gradient in x and y direction, the magnitude and direction of the gradient vector will be calculated using:\n",
    "$$\\text{magnitude} = \\sqrt{G_x^2 + G_y^2}$$\n",
    "$$\\text{direction} = \\arctan\\left(\\frac{G_y}{G_x}\\right)$$\n",
    "\n",
    "Here, it should be noted that arctan has a range of values from -90 to 90 degrees, which does not cover a full circle of 360 degrees. In practice, the function arctan2 is often used, which has a range of values from -180 degrees to 180 degrees, thus allowing a bijective mapping to 0-360 degrees. For the example, the calculation looks as follows:\n",
    "$$\\sqrt{G_x^2 + G_y^2} = \\sqrt{30^2 + 50^2} \\approx 58.31 $$\n",
    "\n",
    "and the direction would be:\n",
    "$$\\arctan\\left(\\frac{50}{30}\\right) \\approx 59.04 \\degree $$\n",
    "\n",
    "This calculation is performed for each pixel in the 8x8 grid, resulting in an 8x8 matrix for the magnitude and an 8x8 matrix for the direction of the gradient vectors. The border is a special case that needs to be addressed (i.e. by using padding). If the image has colors, the calculation is performed for each color channel of a pixel, and the gradient vector with the greatest magnitude is selected from the color channels. The direction of the selected vector is then assigned to the 8x8 direction matrix, and the magnitude of the selected vector is assigned to the 8x8 magnitude matrix for this pixel.\n",
    "\n",
    "#### Step 3 - Create Histogram of Oriented Gradients\n",
    "\n",
    "The next step involves creating histograms from the 8x8 matrices of magnitude and direction for all 8x8 blocks obtained in Step 2.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/HOGDia.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "On the x-axis are the various directions of the gradient vectors of the respective pixels within an 8x8 block, and on the y-axis is the sum of the magnitude of the gradient vectors for each direction. Usually, only directions between 0-180 degrees are considered, and anything beyond is reduced to this interval due to the symmetry of the gradient. The symmetry of the gradient implies that a strong change in intensity within the range of 180-360 degrees only differs in sign from a strong change in intensity within the range of 0-180 degrees. This means that angles greater than 180 degrees can be brought into the interval between 0-180 degrees by subtracting 180 degrees beforehand without losing important information. The 180 degrees are divided into 9 different bins (0, 20, 40, 60, 80, 100, 120, 140, 160) on the x-axis, and the calculation of the magnitude for these bins is as follows:\n",
    "\n",
    "Case 1) Precise Allocation Possible\n",
    "If precise allocation into a bin is possible (e.g., if a pixel has a magnitude of 50 and a direction of 20 degrees), then 50 is added to the sum of the 20-degree bin.\n",
    "\n",
    "Case 2) Precise Allocation Not Possible\n",
    "If precise allocation into a bin is not possible (e.g., if a pixel has a magnitude of 50 and a direction of 30 degrees), then the proximity of the pixel to the classes between which it lies (here 20 and 40) is taken as a weight (here, $\\frac{1}{2}$ each, as 30 is exactly between 20 and 40). The weight is multiplied by the magnitude and added to the sum of the respective bin. In this example, $\\frac{1}{2} \\cdot 50 = 25$ is added to the sum of both the 20-degree and 40-degree bins.\n",
    "\n",
    "Case 3) Angle Between 160 and 180 Degrees:\n",
    "In this case, everything operates similarly to Case 2), with the difference that even though the proximity of the pixel between the classes of 160 and 180 is calculated, the result of $\\text{weight for 180} \\cdot \\text{magnitude of the vector}$ is added to the sum of the bin in class 0 due to symmetry. However, the result of $\\text{weight for 160} \\cdot \\text{magnitude of the vector}$ is added to the sum of the bin in class 160, similar to Case 2).\n",
    "\n",
    "When performing this calculation for each pixel of the 8x8 block, the resulting output is the histogram. This histogram can be transformed into a 9x1 vector containing the weighted sum of magnitudes as entries. For an image with dimensions of 64x128, divided into an 8x16 grid of 8x8 blocks, there would then be $8 \\cdot 16 = 128$ such 9x1 vectors.\n",
    "\n",
    "#### Step 4 - Normalise Histogram Vectors\n",
    "\n",
    "The gradient of an image is sensitive to the overall illumination of the image. When darkening the image (e.g., by halving the intensity values), the length of the gradient vector also halves, resulting in the values in the histogram being halved as well. However, a face should not have different features with half the intensity, which is why the vector needs to be normalized. For normalization, Dalal and Triggs tested various methods. A typical method frequently used for HOG nowadays constructs a 16x16 block from four 8x8 blocks and combines the information into a 36x1 vector (four 9x1 vectors). This vector with 36 entries ($v_1$ to $v_{36}$) is normalized using the L2-norm:\n",
    "\n",
    "$$\\text{magnitude} = \\sqrt{v_1^2 + v_2^2 + ..... + v_{36}^2}$$\n",
    "$$\\text{normalised vector} = [\\frac{v_1}{\\text{magnitude}}, \\frac{v_2}{\\text{magnitude}}, ....., \\frac{v_{36}}{\\text{magnitude}}]$$\n",
    "\n",
    "To extract information from the entire image with dimensions of 64x128, divided into an 8x16 grid of 8x8 blocks, the 16x16 block is first placed at the top left of the image. Then, the block is moved from left to right with a step size of 1 through the entire row of the image. Once a row is completed, the process continues with the next row, iterating until the block traverses the entire image (similar to a sliding window). The block can be shifted a total of 7 times per row and 15 times downwards, resulting in performing $7 \\cdot 15$ computations that yield a 36x1 vector as a result. Thus, a total of $7 \\cdot 15 \\cdot 36 \\cdot 1 = 3780$ different entries are obtained, which are transformed into a 3780x1 vector and then passed on to a classifier (e.g., a Support Vector Machine (SVM)). Before passing it to an SVM, this vector probably has to be reduced (to prevent overfitting) using for instance PCA (Principal Component Analysis). However, this will not be explained in this article.\n",
    "\n",
    "#### SVM with HOG features\n",
    "\n",
    "The resulting vector from the HOG algorithm, that was potentially reduced using PCA, is often fed to an SVM. The SVM tries to find a hyperplane that best separates the datapoints of different classes in a high-dimensional space. On a basic level, the datapoints can be classified into images that contain a face (positive samples), and images that don't contain a face (negative samples). The HOG features extracted from negative and positive samples can then be used to train the SVM so that it learns to distinguish between images that contain faces and ones that don't. Additionally, a trained SVM can be used as a sliding window that analyses a small part of a predefined size of the image to determine whether this part contains a face or not. This allows to not only classify images with faces correctly, but also to detect faces on the image.\n",
    "\n",
    "#### Practical Application\n",
    "\n",
    "Our python implementation for Hog-SVM is using the following Code in the backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed883d4b7da44a1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-14T09:31:03.808832600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def highlight_face_hog_svm(img: Image):\n",
    "    img = cv2.cvtColor(numpy.array(img), cv2.COLOR_RGB2BGR)\n",
    "    gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = hog_svm_detector(gray_image)\n",
    "\n",
    "    for face in faces:\n",
    "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 255, 0), 6)\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(img_rgb), len(faces), '?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273e582607a99d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This implementation of Hog-SVM face detection processes a given PIL Image object. It converts the image to BGR format, then to grayscale. Using a pre-trained hog_svm_detector, it detects faces in the grayscale image. This pre-trained detector is already provided by the library, so we do not need to train or create our own. Detected faces are outlined with turquoise rectangles, and the modified image is converted back to RGB format before being returned. \n",
    "\n",
    "Here is an example output of the algorithm:\n",
    "<div>\n",
    "<img src=\"images/detected-faces-examples/detected_face_hog_svm.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffede5b2ad0829e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### MTCNN\n",
    "\n",
    "A more recently used approach is the MTCNN face detection method. MTCNN stands for Multi-task Cascaded Convolutional Networks and, as the name suggests, is based on Convolution Neural Networks. As the algorithm, like Viola-Jones, has a cascaded structure and can therefore exclude non-face regions at an early stage, the method is suitable for real-time face detection. MTCNN basically consists of 3 different steps:\n",
    "1. Face Detection\n",
    "2. Facial Landmark Detection\n",
    "3. Face Classification\n",
    "\n",
    "In the following, we will give a brief overview of the steps in MTCNN.\n",
    "\n",
    "#### Step 1 - Face Detection\n",
    "\n",
    "In the first step, the MTCNN recognizes potential candidate faces in the input image. It uses a cascade of convolutional networks to filter out regions that are unlikely to contain a face and focuses on regions with a higher probability of containing a face.\n",
    "The cascade structure comprises several stages consisting of different CNNs. At each stage, the network limits the number of eligible regions by the result of the CNN.\n",
    "The end result of this step is a series of bounding boxes that represent the potential face regions in the image.\n",
    "\n",
    "#### Step 2 - Facial Landmark Detection\n",
    "Once the potential facial regions are identified, the second step of MTCNN is responsible for locating facial keypoints within each bounding box.\n",
    "Facial keypoints are specific points on the face, such as the eyes, nose and mouth. These landmarks are critical for tasks such as facial alignment and detection.\n",
    "The network at this step is designed to regress the coordinates of these facial features for each recognized face.\n",
    "\n",
    "#### Step 3 - Face Classification\n",
    "\n",
    "The third step of MTCNN deals with the classification of each bounding box as face or non-face. This step helps to eliminate false positives and improves the accuracy of the overall face recognition system.\n",
    "A classifier is trained to distinguish between faces and non-faces by extracting features from the candidate regions. \n",
    "The result of this step is a refined set of bounding boxes, with the corresponding face keypoints, which are more likely to contain actual faces.\n",
    "\n",
    "#### Practical Application\n",
    "\n",
    "Our python implementation for MTCNN is using the following Code in the backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdadea1ca88b25",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-14T09:31:03.809831Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def highlight_face_mtcnn(img: Image):\n",
    "    img = numpy.array(img)\n",
    "    # Disable printing\n",
    "    with io.StringIO() as dummy_stdout:\n",
    "        with redirect_stdout(dummy_stdout):\n",
    "            faces = mtcnn_detector.detect_faces(img)\n",
    "\n",
    "    confidence = 100\n",
    "\n",
    "    for face in faces:\n",
    "        confidence = round(face['confidence'] * 100, 3)\n",
    "        x, y, w, h = face['box'][0], face['box'][1], face['box'][2], face['box'][3]\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 255), 6)\n",
    "\n",
    "    return Image.fromarray(img), len(faces), confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3f1aa01a381316",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This implementation of MTCNN face detection processes a given PIL Image object. It converts the image to a numpy array. Using a pre-trained mtcnn detector, it detects faces in the image. This pre-trained detector is already provided by the library, so we do not need to train or create our own. Detected faces are outlined with pink rectangles, and are then being returned.\n",
    "\n",
    "Here is an example output of the algorithm:\n",
    "<div>\n",
    "<img src=\"images/detected-faces-examples/detected_face_mtcnn.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559dbfc00f0de7a8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### SSD\n",
    "\n",
    "Another approach that has been used more recently is the SSD face detection method. SSD stands for Single-Shot Multibox Detector. It is actually an approach for object recognition, but can also be used for face detection. Just like MTCNN, SSD is based on a CNN that is used for feature extraction. The ability to perform all steps of face detection in a single pass makes this method suitable for real-time face detection. The result of SSD is multiple bounding boxes with potential faces that need to be evaluated against a confidence threshold.\n",
    "\n",
    "#### Practical Application\n",
    "\n",
    "Our python implementation for SSD is using the following Code in the backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd221c05db772d7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-14T09:31:03.811831300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def highlight_face_ssd(img: Image):\n",
    "    img = cv2.cvtColor(numpy.array(img), cv2.COLOR_RGB2BGR)\n",
    "    resized_rgb_image = cv2.resize(img, (300, 300))\n",
    "    imageBlob = cv2.dnn.blobFromImage(image=resized_rgb_image)\n",
    "    ssd_detector.setInput(imageBlob)\n",
    "    detections = ssd_detector.forward()\n",
    "\n",
    "    confidence = 100\n",
    "    number_of_faces = 0\n",
    "\n",
    "    # only show detections over 80% certainty\n",
    "    for row in detections[0][0]:\n",
    "        if row[2] > 0.80:\n",
    "            confidence = round(row[2] * 100, 3)\n",
    "            number_of_faces += 1\n",
    "            x1, y1, x2, y2 = int(row[3] * img.shape[1]), int(row[4] * img.shape[0]), int(row[5] * img.shape[1]), int(\n",
    "                row[6] * img.shape[0])\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 255), 6)\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(img_rgb), number_of_faces, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89875c318647d6e9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This implementation of SSD face detection processes a given PIL Image object. It converts the image to BGR format and then resized to 300x300 pixels. Using a pre-trained ssd detector, it detects faces in the image. This pre-trained detector is already provided by the library, so we do not need to train or create our own. The detector provides a result for each box position, so we have to sort out unwanted results. Detected faces are outlined with yellow rectangles, and are then being returned.\n",
    "\n",
    "Here is an example output of the algorithm:\n",
    "<div>\n",
    "<img src=\"images/detected-faces-examples/detected_face_ssd.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9120f354b73b59f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Filter\n",
    "\n",
    "This part of the documentation will introduce the filters and investigate how precise filters can prevent HOG and SVM from detecting faces successfully. HOG and SVM was chosen because we currently determine the keypoints of the faces with this approach. The images, which were used during this section, stem from the Labeled Faces in the Wild (LFW) dataset. The dataset contains more than 13,000 images of 5,749 people. However, we focused exclusively on individuals for whom a minimum of 100 facial images were available, leading to 1140 final images. Without any modifications, the algorithm detects 1099/1140 faces.\n",
    "\n",
    "### Cow Face\n",
    "\n",
    "This filter applies a cow pattern overlay to the facial area of the image. It calculates the bounding box for the face using facial keypoints and resizes the cow pattern to fit this area. The pattern is then applied over the face with a specified level of transparency (alpha_of_cow_pattern), creating a cow-patterned effect on the facial area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5ceea146a9515c",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2547487d41ca91",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-14T09:31:03.812833500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_cow_pattern(image: Image, keypoints, alpha_of_cow_pattern: int = 85) -> Image:\n",
    "    foreground = Image.open('../backend/filters/cow_pattern.png').convert(\"RGBA\")\n",
    "    foreground_parts = Image.new('RGBA', image.size)\n",
    "    for (box, face_keypoints, face_shape_landmarks, _) in keypoints:\n",
    "        (minX, maxX), (minY, maxY), (width, height) = calculate_face_shape_landmarks_box_positions(face_shape_landmarks)\n",
    "        new_foreground_part = foreground.resize((width, height), resample=Image.LANCZOS)\n",
    "        foreground_parts.paste(new_foreground_part, (minX, minY), new_foreground_part)\n",
    "    foreground_parts.putalpha(alpha_of_cow_pattern)\n",
    "    image = apply_filter_on_faces(image, keypoints, foreground_parts)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9447b785f2a0ca16",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The idea behind the Cow Face Filter is that it falsifies the magnitude and direction of the gradient vectors in the face, which HOG uses to extract features. The pattern has strong intensity changes (black and white) that will create gradient vectors with a significantly bigger magnitude than the vectors of the original face. We used different alpha values for the pattern, to see how it affects the detection. An example with an alpha value of 45 can be seen below:\n",
    "<div>\n",
    "<img src=\"images/CowMaskwithAlphaof45.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Using the LFW dataset with different alpha values yields the following result:\n",
    "<div>\n",
    "<img src=\"images/CowMaskModification.png\" width=\"500\"/>\n",
    "</div>\n",
    "This approach does work for preventing face detection, although it also significantly alters the facial features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea3a6d6959b8afc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Salt and Pepper Filter\n",
    "\n",
    "This filter generates a 'salt and pepper' noise effect and applies it over the facial area. The noise is created by randomly assigning black and white pixels in equal proportions and then resizing this noise pattern to fit the face. The pattern is applied with a specified alpha value (alpha_of_salt_n_pepper), overlaying the face with this distinctive noise effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb8181cd2cec586",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-14T09:31:03.814340Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_salt_n_pepper(image: Image, keypoints, alpha_of_salt_n_pepper: int = 90) -> Image:\n",
    "    foreground_parts = Image.new('RGBA', image.size)\n",
    "    for (box, face_keypoints, face_shape_landmarks, _) in keypoints:\n",
    "        (minX, maxX), (minY, maxY), (width, height) = calculate_face_shape_landmarks_box_positions(face_shape_landmarks)\n",
    "        pixels = np.zeros(width * height, dtype=np.uint8)\n",
    "        pixels[:width * height // 2] = 255  # Set first half to white (value 255)\n",
    "        np.random.shuffle(pixels)\n",
    "        rgb_box = np.stack((pixels, pixels, pixels), axis=-1)\n",
    "        rgb_box_reshaped = np.reshape(rgb_box, (height, width, 3))\n",
    "        rgb_box_image = Image.fromarray(rgb_box_reshaped)\n",
    "        rgb_box_image.putalpha(255)\n",
    "        foreground_parts.paste(rgb_box_image, (minX, minY), rgb_box_image)\n",
    "    foreground_parts.putalpha(alpha_of_salt_n_pepper)\n",
    "    image = apply_filter_on_faces(image, keypoints, foreground_parts)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd51a851faa7974c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The Salt and Pepper Filter, which has an additional alpha value for the transparency of the salt and pepper pattern, was based on the same idea as the Cow Face Filter. An example with an alpha value of 45 can be seen below:\n",
    "<div>\n",
    "<img src=\"images/SaltandPepperwithalphaof45.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Using the LFW dataset with different alpha values yields the following result:\n",
    "<div>\n",
    "<img src=\"images/SaltandPepperModification.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Since the salt and pepper is very similar to the cow pattern, the resulting diagram is almost identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af92164fcf0582e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pixelate\n",
    "\n",
    "This filter creates a pixelation effect by initially reducing the image's resolution and then scaling it back to its original size. The initial downscaling reduces detail, creating larger 'blocks' of color, and the subsequent upscaling maintains this blocky appearance. The degree of pixelation is dictated by the pixel_size parameter, with larger values producing more pronounced pixelation. The effect can be applied to the whole image or focused on a specific area, like the face, as determined by keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca531f78ab900d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-14T09:31:03.815346Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_pixelate(image: Image, keypoints, only_face=True, pixel_size=10) -> Image:\n",
    "    small = image.resize((image.size[0] // pixel_size, image.size[1] // pixel_size), Image.NEAREST)\n",
    "    modified_image = small.resize(image.size, Image.NEAREST)\n",
    "    if only_face:\n",
    "        return swap_images_at_face_position(image, keypoints, modified_image)\n",
    "    else:\n",
    "        return modified_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40426afb45ca0c60",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The Pixelate filter pixelates the image. On the website, it can be applied only to the face or to the entire image. Since the LFW dataset only contains faces, the filter was applied to the entire image. An example with a pixel size of 2 can be seen below:\n",
    "<div>\n",
    "<img src=\"images/Pixelate.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Using the LFW dataset with different pixel size values yields the following result:\n",
    "<div>\n",
    "<img src=\"images/PixelateModification.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "For low pixel size values, the number of detected faces is high and the face is still recognizable. With increasing pixel size values, the face is barely recognizable and the number of detected faces is declining.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40492cb733d15d74",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Blur\n",
    "\n",
    "The function takes a NumPy array representing an image as input. It converts the array to a PIL Image, applies a Box Blur filter with a radius of 10 to the image using the filter() method, and then converts the modified PIL Image back to a NumPy array before returning the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d57bc21cf2d05ec0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-14T09:31:03.817351800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_blur\u001b[39m(image: \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m      2\u001b[0m     pilImage \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(image)\n\u001b[0;32m      3\u001b[0m     pilImage \u001b[38;5;241m=\u001b[39m pilImage\u001b[38;5;241m.\u001b[39mfilter(ImageFilter\u001b[38;5;241m.\u001b[39mBoxBlur(\u001b[38;5;241m10\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def apply_blur(image: np.ndarray):\n",
    "    pilImage = Image.fromarray(image)\n",
    "    pilImage = pilImage.filter(ImageFilter.BoxBlur(10))\n",
    "    return np.array(pilImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84b54f8352a979b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The Blur Filter applies a box blur effect to the image. Similarly to the Pixelate filter, it can be applied only to the face or to the entire image. Since the LFW dataset only contains faces, the filter was applied to the entire image. An example with a radius of 1 can be seen below:\n",
    "<div>\n",
    "<img src=\"images/BoxBlur.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Using the LFW dataset with a radius of 1 yields the following result:\n",
    "<div>\n",
    "<img src=\"images/BoxBlurModification.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "The algorithm still detects 1090/1140 faces after the modification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d6281a02b3bdd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Sunglasses\n",
    "\n",
    "This filter overlays sunglasses onto the face in an image. It identifies the positions of the left and right eyes using facial keypoints and calculates the angle between them to rotate the sunglasses image accordingly. The distance between the eyes is used to dynamically scale the sunglasses' size, ensuring they fit the face proportionally. The sunglasses image is resized and rotated before being superimposed onto the face, creating the appearance of the subject wearing sunglasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7530356ffecc6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-14T09:31:03.839373500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_sunglasses(image: Image, keypoints, scale_factor: float = 2.5) -> Image:\n",
    "    foreground = Image.open('filters/sunglasses.png')\n",
    "    for (box, face_keypoints, face_shape_landmarks, _) in keypoints:\n",
    "        left_eye = face_keypoints['left_eye']\n",
    "        right_eye = face_keypoints['right_eye']\n",
    "        dx = right_eye[0] - left_eye[0]\n",
    "        dy = right_eye[1] - left_eye[1]\n",
    "        angle_radians = math.atan2(-dy, dx)\n",
    "        angle_degrees = math.degrees(angle_radians)\n",
    "        eye_distance = math.dist((right_eye[0], right_eye[1]), (left_eye[0], left_eye[1]))\n",
    "        foreground_width_to_height_ratio = foreground.size[0] / foreground.size[1]\n",
    "        foreground = foreground.resize(size=(\n",
    "            int(scale_factor * eye_distance), int(scale_factor * eye_distance / foreground_width_to_height_ratio)))\n",
    "        rotated_overlay = foreground.rotate(angle_degrees, expand=True)\n",
    "        left_part = (scale_factor - 1) / 2\n",
    "        left_upper_sunglasses = (int(left_eye[0] - eye_distance * left_part),\n",
    "                                 int(left_eye[1] - eye_distance * left_part / foreground_width_to_height_ratio))\n",
    "        left_upper_paste = (left_upper_sunglasses[0], int(left_upper_sunglasses[1] - math.fabs(\n",
    "            math.cos(math.radians(90 - angle_degrees)) * scale_factor * eye_distance)))\n",
    "        image.paste(rotated_overlay, left_upper_paste, rotated_overlay)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defcd1b6519158c7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A more artistic approach, is the application of sunglasses on the eyes of a given face:\n",
    "<div>\n",
    "<img src=\"images/SunglassesonFace.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Using the LFW dataset with different alpha values yields the following result:\n",
    "<div>\n",
    "<img src=\"images/SunglassesonFaceModification.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Since this approach does not nearly cover enough area of the face and is rather artistic, the result is not surprising to us. 1000 of the 1140 faces were successfully discovered after the modification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232c79636b53866",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Medicine Mask\n",
    "\n",
    "This filter overlays a medical-style mask image onto the face. It locates the position of the mouth and nose using facial keypoints and adjusts the size and rotation of the mask image to align with these features. The mask is placed to cover the lower half of the face, resembling the appearance of wearing a medical mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c970b94cbbca3f8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-14T09:31:03.841375700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_medicine_mask(image: Image, keypoints) -> Image:\n",
    "    foreground = Image.open('filters/medicine_mask.png').convert(\"RGBA\")\n",
    "    for (box, face_keypoints, face_shape_landmarks, _) in keypoints:\n",
    "        left_mouth = face_keypoints['left_mouth']\n",
    "        right_mouth = face_keypoints['right_mouth']\n",
    "        dx = right_mouth[0] - left_mouth[0]\n",
    "        dy = right_mouth[1] - left_mouth[1]\n",
    "        angle_radians = math.atan2(-dy, dx)\n",
    "        angle_degrees = math.degrees(angle_radians)\n",
    "        face_width = box[2]\n",
    "        foreground_width_to_height_ratio = foreground.size[0] / foreground.size[1]\n",
    "        foreground = foreground.resize(size=(face_width, int(face_width / foreground_width_to_height_ratio)))\n",
    "        rotated_overlay = foreground.rotate(angle_degrees, expand=True)\n",
    "        left_upper_face_mask = (box[0], face_keypoints['nose'][1])\n",
    "        left_upper_paste = (left_upper_face_mask[0], int(left_upper_face_mask[1] - math.fabs(\n",
    "            math.cos(math.radians(90 - angle_degrees)) * face_width)))\n",
    "        image.paste(rotated_overlay, left_upper_paste, rotated_overlay)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81285c197f83cfba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Another artistic approach is the application of a medicine mask to the face:\n",
    "<div>\n",
    "<img src=\"images/MedicineMaskExample.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Using the LFW dataset with different alpha values yields the following result:\n",
    "<div>\n",
    "<img src=\"images/MedicineMaskModification.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "This approach covers more area of the face. It leads to a considerable change (120 out of 1140 were detected) in comparison to the Sunglasses approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ad73b5a17dea45",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Hiding Faces\n",
    "\n",
    "This filter randomly places multiple small face mask images over the image. It ensures that these masks do not overlap with the facial areas identified by the keypoints. Each mask is resized according to specified dimensions (`face_mask_width` and `face_mask_height`) and applied with a certain level of transparency (`alpha_of_masks`). This creates a scattered mask effect across the image, avoiding the actual facial areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e34ed4ba48e250",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-14T09:31:03.842373800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_hide_with_masks(img: Image, keypoints, number_of_masks: int = 40,\n",
    "                          face_mask_width: int = 75, face_mask_height: int = 75,\n",
    "                          alpha_of_masks: int = 45) -> Image:\n",
    "    foreground = Image.open('../backend/filters/whole_face_mask.png').convert('RGBA')\n",
    "    foreground_alpha = apply_alpha_to_transparent_image(foreground, alpha_of_masks)\n",
    "    face_and_mask_coordinates = find_face_rectangles_mtcnn(keypoints)\n",
    "    mask_cords = find_free_coordinates_outside_of_rectangles(img, number_of_masks, face_mask_width, face_mask_height,\n",
    "                                                             face_and_mask_coordinates)\n",
    "    for mask_coords in mask_cords:\n",
    "        resized_foreground = foreground_alpha.resize((face_mask_width, face_mask_height), resample=Image.LANCZOS)\n",
    "        img.paste(resized_foreground, (mask_coords[0], mask_coords[1]), resized_foreground)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e916c492adecc9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The goal of this approach is to add additional artifacts to the image, that should erroneously be classified as face by classifiers. An example would be adding face masks to the image and making them barely visible with a low alpha value:\n",
    "<div>\n",
    "<img src=\"images/HideWithMaskExample.png\" width=\"900\"/>\n",
    "</div>\n",
    "\n",
    "This approach only slightly modifies the face (for low alpha values), and it increases the false positives of the classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49513190f916b10b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Face Recognition\n",
    "\n",
    "After we had completed the part with face detection, we wanted to deal with the area of face recognition. Face recognition is simpler than face detection, as a face must first be detected before it can be recognised. But what is face recognition?\n",
    "Face recognition is the task of recognising faces. However, recognising faces can mean several things. On the one hand, it can mean that faces can be assigned to known persons. These known persons are then stored in a database and for each picture of a face it can then be said whether it is, for example, Olaf Scholz or Christian Lindner. Another method of recognition is that you don't even know which person a face belongs to, but you can say that two faces belong to the same person.\n",
    "<div>\n",
    "<img src=\"images/FaceRecognitionDatabase.png\" width=\"900\"/>\n",
    "</div>\n",
    "\n",
    "As we don't want to create a large database that might have to store sensitive data, we decided that our WebApp should only be able to tell whether two pictures show the same person.\n",
    "To do this, we used the [face-recognition library](https://github.com/ageitgey/face_recognition) . This library offers many useful methods for face recognition. The library uses the face recognition model from [dlib](http://dlib.net/files/dlib_face_recognition_resnet_model_v1.dat.bz2) to compare the faces. The model calculates a 128D vector from the keypoints of the face, which represents the characteristics of the face. Two of these vectors can then be checked for similarity using the Euclidean distance. If the distance is below a defined threshold value, it is assumed that the person is the same. The model has achieved an accuracy of 99.38% on the standard LFW face recognition benchmark. Dlib also provides a small [demo](http://dlib.net/face_recognition.py.html) if you want to test the model yourself.\n",
    "\n",
    "The face-recognition library provides two different methods for face recognition. The first method is based on five keypoints, taking only two keypoints from both eyes and one keypoint from the nose. The other method is based on 68 keypoints and uses the same keypoints that we use for the filters. As the difference in speed is very small, but the accuracy of 68 keypoints was better, we decided in favour of the 68 keypoints approach.\n",
    "\n",
    "The whole process is shown in this diagram:\n",
    "<div>\n",
    "<img src=\"images/FaceRecognitionProcess.png\" width=\"900\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ca47fbdfa263d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Practical Application\n",
    "\n",
    "Our python implementation for the face recognition is using the following Code in the backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289054ec09491e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T17:59:34.214789500Z",
     "start_time": "2024-02-13T17:59:34.086953Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (341758604.py, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 41\u001b[1;36m\u001b[0m\n\u001b[1;33m    return Image.fromarray(orig_img_rgb), Image.fromarray(mod_img_rgb), count_of_matches Code\u001b[0m\n\u001b[1;37m                                                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def recognize_faces(orig_img: Image, mod_img: Image, orig_keypoints):\n",
    "    orig_img_bgr = cv2.cvtColor(np.array(orig_img), cv2.COLOR_RGB2BGR)\n",
    "    mod_img_bgr = cv2.cvtColor(np.array(mod_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    gray_image = cv2.cvtColor(np.asarray(mod_img_bgr), cv2.COLOR_BGR2GRAY)\n",
    "    faces = hog_svm_detector(gray_image)\n",
    "    face_encodings_unknown = []\n",
    "    boxes_mod = []\n",
    "    for face in faces:\n",
    "        box = [face.left(), face.top(), face.width(), face.height()]\n",
    "        face_encodings_unknown.append(np.array(calculate_face_encoding(np.asarray(mod_img_bgr), box)))\n",
    "        boxes_mod.append((face.left(), face.top(), face.width(), face.height()))\n",
    "\n",
    "    face_encodings_orig = []\n",
    "    boxes_orig = []\n",
    "    for box, _, _, face_encoding_orig in orig_keypoints:\n",
    "        face_encodings_orig.append(np.array(face_encoding_orig))\n",
    "        boxes_orig.append(box)\n",
    "\n",
    "    count_of_matches = 0\n",
    "\n",
    "    for j, face_encoding_unknown in enumerate(face_encodings_unknown):\n",
    "        matches = face_recognition.compare_faces(face_encodings_orig, face_encoding_unknown, tolerance=0.55)\n",
    "\n",
    "        for i, match in enumerate(matches):\n",
    "            if match:\n",
    "                count_of_matches += 1\n",
    "                selected_color = palette[(j * 2) % num_colors]\n",
    "                bgr_color = tuple(int(value * 255) for value in selected_color)\n",
    "                box_orig = boxes_orig[i]\n",
    "                box_mod = boxes_mod[j]\n",
    "                top, right, bottom, left = box_orig[1], box_orig[0] + box_orig[2], box_orig[1] + box_orig[3], box_orig[\n",
    "                    0]\n",
    "                cv2.rectangle(orig_img_bgr, (left, top), (right, bottom), bgr_color, 6)\n",
    "\n",
    "                top, right, bottom, left = box_mod[1], box_mod[0] + box_mod[2], box_mod[1] + box_mod[3], box_mod[0]\n",
    "                cv2.rectangle(mod_img_bgr, (left, top), (right, bottom), bgr_color, 6)\n",
    "\n",
    "    orig_img_rgb = cv2.cvtColor(orig_img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    mod_img_rgb = cv2.cvtColor(mod_img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(orig_img_rgb), Image.fromarray(mod_img_rgb), count_of_matches Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402ba0cabf69b2a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This implementation takes an original image with the calculated keypoints, which also contains the 128D vector of the characteristics of the face and a potential modified image that is compared to the original image. The modified image is pre-processed and then the face bounding boxes are calculated using the svm+hog face detection algorithm. The bounding boxes are then used to compute the 128D vector of the faces found in the image. \n",
    "\n",
    "Each face found in the modified image is then compared to each face in the original image. If the Euclidean distance is less than the given tolerance of 0.55, a match is registered. For each match, a pair of colored boxes is drawn in both of the images where the matching faces are. The number of pairs is also counted.\n",
    "\n",
    "Here is an example output of the algorithm:\n",
    "<div>\n",
    "<img src=\"images/detected-faces-examples/example_face_recognition.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0ec8036dd9835",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Modification\n",
    "\n",
    "This part of the documentation will investigate how precise filters can prevent the face recognition. The images, which were used during this section, stem from the Labeled Faces in the Wild (LFW) dataset. The dataset contains more than 13,000 images of 5,749 people. In this section, we only consider pairs of two images containing the same person, resulting in 1100 final image pairs. Without any modifications, the algorithm recognizes 918/1100 faces.\n",
    "\n",
    "### Filters\n",
    "\n",
    "The filters we use are based on those already used for face detection and new filters specifically designed to prevent face recognition. The description of the face detection filters can be found in the face detection section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddfa2df",
   "metadata": {},
   "source": [
    "### Morphing\n",
    "We used the following morphing function in the next three filters to modify the original image. The function performs localized morphing on an image using specified keypoints. It takes an image (`image_cv`), a set of keypoints, a `radius` defining the area of effect around each keypoint, and a `morph_strength` value to control the intensity of the morph.\n",
    "\n",
    "For each keypoint, the function creates a grid of coordinates covering the image. It calculates the distance from each grid point to the keypoint and applies a circular mask based on the specified radius. This mask isolates the effect to a circular area around the keypoint. A displacement field is then computed, diminishing exponentially with distance from the keypoint and scaled by `morph_strength`. This field dictates how much each point in the masked area is moved, creating the morphing effect.\n",
    "\n",
    "The displacement is applied to the grid points, shifting them in a manner that reflects the direction and magnitude of the displacement field. The OpenCV function cv2.remap is used to remap the image based on the adjusted grid coordinates, effectively warping the image around each keypoint. This process is repeated for all keypoints, with the cumulative effect resulting in a complex morphing of the image. The function finally returns the morphed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdcc611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def morph_image(image_cv, keypoints, radius, morph_strength):\n",
    "    h, w = image_cv.shape[:2]\n",
    "    for point in keypoints:\n",
    "        x_grid, y_grid = np.meshgrid(np.arange(w), np.arange(h))\n",
    "        dx = x_grid - point[0]\n",
    "        dy = y_grid - point[1]\n",
    "        distance = np.sqrt(dx ** 2 + dy ** 2)\n",
    "        mask = np.where(distance < radius, 1, 0)\n",
    "        displacement = np.exp(-distance / radius) * morph_strength\n",
    "        displacement *= mask\n",
    "        map_x = x_grid + displacement * np.sign(dx)\n",
    "        map_y = y_grid + displacement * np.sign(dy)\n",
    "        image_cv = cv2.remap(image_cv, map_x.astype(np.float32), map_y.astype(np.float32), cv2.INTER_LINEAR)\n",
    "    return image_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5fa8676f29a56",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Morph Eyes\n",
    "\n",
    "This filter specifically targets the eyes in an image to apply a morphing effect. It first checks for the presence of facial keypoints and, if found, identifies the positions of the left and right eyes. The filter then applies a morphing algorithm to these eye regions. The radius parameter, which is dynamically set based on the size of the detected face, defines the area around each eye that is affected by the morphing. The morph_strength parameter controls the intensity of the morphing effect. The result is a transformation of the eye regions, creating a unique, modified appearance of the eyes in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a88b02dd96c582",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-13T17:59:34.104904600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_morph_eyes(image: Image, keypoints, radius=75, morph_strength=10.0) -> Image:\n",
    "    if len(keypoints) == 0:\n",
    "        return image\n",
    "    image_cv = np.array(image)\n",
    "    image_cv = cv2.cvtColor(image_cv, cv2.COLOR_RGB2BGR)\n",
    "    eye_points = [face_keypoints[key] for _, face_keypoints, _, _ in keypoints for key in ['left_eye', 'right_eye']]\n",
    "    radius = keypoints[0][0][2] / 3\n",
    "    image_cv = morph_image(image_cv, eye_points, radius, morph_strength)\n",
    "    return Image.fromarray(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a25466b178e672",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div>\n",
    "<img src=\"images/MorphEyeswithmorphstrengthof5.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc04044a841c86f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Morph Mouth\n",
    "\n",
    "This filter is designed to morph the mouth area of an image. After ensuring that facial keypoints are present, it locates the positions of the left and right mouth corners and the nose. These points define the region around the mouth to be morphed. Similar to the Morph Eyes Filter, the radius for the morphing effect is determined based on the face size, specifically set to half the radius used for the eyes. The morph_strength parameter adjusts the level of morphing applied. This filter alters the mouth region, modifying its appearance in a distinctive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56ab5dff0dde52",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-13T17:59:34.107897Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_morph_mouth(image: Image, keypoints, radius=75, morph_strength=10.0) -> Image:\n",
    "    if len(keypoints) == 0:\n",
    "        return image\n",
    "    image_cv = np.array(image)\n",
    "    image_cv = cv2.cvtColor(image_cv, cv2.COLOR_RGB2BGR)\n",
    "    mouth_points = [face_keypoints[key] for _, face_keypoints, _, _ in keypoints for key in\n",
    "                    ['left_mouth', 'right_mouth', 'nose']]\n",
    "    radius = keypoints[0][0][2] / 6\n",
    "    image_cv = morph_image(image_cv, mouth_points, radius, morph_strength)\n",
    "    return Image.fromarray(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f7f59c0fef56a8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div>\n",
    "<img src=\"images/MorphMouthwithmorphstrengthof5.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67221382e1559ebc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Morph All\n",
    "\n",
    "This filter applies a comprehensive morphing effect to all key facial features. It utilizes all the detected facial keypoints, including points defining the face outline. The filter calculates a radius that is smaller compared to the previous filters, as it applies the morphing effect more broadly across the face. The morph_strength parameter still controls the intensity of the morphing. This all-encompassing approach results in a more dramatic transformation of the entire face, altering multiple features simultaneously for a significant visual change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b41f64523f2fced",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-13T17:59:34.119414Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_morph_all\u001b[39m(image: \u001b[43mImage\u001b[49m, keypoints, radius\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m75\u001b[39m, morph_strength\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10.0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(keypoints) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "def apply_morph_all(image: Image, keypoints, radius=75, morph_strength=10.0) -> Image:\n",
    "    if len(keypoints) == 0:\n",
    "        return image\n",
    "    image_cv = np.array(image)\n",
    "    image_cv = cv2.cvtColor(image_cv, cv2.COLOR_RGB2BGR)\n",
    "    all_points = [point for _, face_keypoints, outline, _ in keypoints for key, point in face_keypoints.items()] + [pt for _, _, outline, _ in keypoints for pt in outline]\n",
    "    radius = keypoints[0][0][2] / 12\n",
    "    image_cv = morph_image(image_cv, all_points, radius, morph_strength)\n",
    "    return Image.fromarray(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bebf9625d68105",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div>\n",
    "<img src=\"images/MorphAllwithmorphstrengthof5.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e6d418c725b540",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Result\n",
    "\n",
    "We only tested our most promising filters which should prevent face recognition without altering the image too much. The result is shown in the bar chart below:\n",
    "<div>\n",
    "<img src=\"images/FaceRecognitionAnalysis.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "The diagram shows that many of the filters achieve good results.\n",
    "The Salt&Pepper and Cowface filters are probably so good because they prevent face detection. The Medicine Mask filter hides the lower keypoints, which is why it's so good. It is also noticeable that the keypoints of the eyes are very important for face recognition, as Morph Eyes is significantly better than Morph Mouth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Web Application\n",
    "We based our project on the provided demo applications. The app is split into a seperate frontend and backend component.\n",
    "The frontend is implemented using Vue and the Vuetify framework.\n",
    "\n",
    "It operates on a simple yet effective principle: users can upload their custom photos to the platform. Once uploaded, they have the option to apply a selection of developed filters. These filters are designed to test their effect on face detection and recognition algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frontend\n",
    "The frontend can be run by installing node and the required dependencies `npm install` afterwards the application is compiled and started through `npm run serve`.\n",
    "\n",
    "The UI is seperated into two tabs, one for FD and one for FR, this allows a differentiation between the two needed user input setups while keeping it on the same loaded page.\n",
    "\n",
    "**FD-Tab**\n",
    "<div>\n",
    "<img src=\"images/WebApp-FD.png\" width=\"900\"/>\n",
    "</div>\n",
    "\n",
    "**FR-Tab**\n",
    "<div>\n",
    "<img src=\"images/WebApp-FR.png\" width=\"900\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backend\n",
    "The backend is structured around the FastAPI framework, designed to provide a robust and efficient interface for our web application. The backend is pivotal in handling the core functionalities of our project, including image processing, filter application, and face detection and recognition algorithms. Below, we outline the various API endpoints incorporated into our backend, each playing a crucial role in our application's functionality.\n",
    "\n",
    "The backend can be run by installing node and the required dependencies `pip install -r .\\requirements.txt` afterwards the application is started through `uvicorn app.main:app --reload`\n",
    "\n",
    "#### Endpoints\n",
    "\n",
    "**`/`** A simple endpoint to verify that the API is online, returning a basic confirmation message.\n",
    "\n",
    "**`/convert-image`** This endpoint accepts an image in base64 format and converts it, ensuring it's in the correct RGB mode for processing.\n",
    "\n",
    "**`/get-filters`** Retrieves the list of available filters with their respective attributes, indicating their applicability to face detection, recognition, and whether they target the face only.\n",
    "\n",
    "**`/get-algorithms`** Provides a list of available face detection algorithms, including Viola-Jones, HOG-SVM, MTCNN, and SSD, each with a distinct approach to detecting faces in images.\n",
    "\n",
    "**`/apply-filter`** Applies a specified filter to an image. It processes the image based on the selected filter, which can range from blurring and pixelation to more complex operations like morphing facial features.\n",
    "\n",
    "**`/run-face-detection`** Runs face detection algorithms on the provided image, employing a multi-threaded approach to process different algorithms concurrently for efficiency.\n",
    "\n",
    "**`/run-face-recognition`** This endpoint is designed to execute face recognition on the original and modified images, assessing the impact of applied filters on recognition accuracy.\n",
    "\n",
    "**`/generate-keypoints`** Initiates the generation of keypoints on the provided image, a crucial step in applying certain filters and for the facial recognition process.\n",
    "\n",
    "#### Modular Filter System\n",
    "To add new filters, you should first add a new entry to the `FILTERS` array in the JSON structure at the top of the backend code. This entry should specify the name of the filter, the display name, the face detection and recognition flags, and whether it applies to the face only. For example:\n",
    "```json\n",
    "{\n",
    "    \"name\": \"newFilterName\",\n",
    "    \"displayName\": \"New Filter Display Name\",\n",
    "    \"faceDetection\": true/false,\n",
    "    \"faceRecognition\": true/false,\n",
    "    \"faceOnly\": true/false\n",
    "}\n",
    "```\n",
    "Then, in the `/apply-filter` method, include the filter's specific processing logic. This method uses a match-case statement to apply the selected filter based on its name. This is where the specific image processing actions for the new filter need to be implemented:\n",
    "```python\n",
    "\n",
    "@app.post('/apply-filter')\n",
    "async def apply_filter(data: ApplyFilterRequestData):\n",
    "   <...>\n",
    "        match data.filter:\n",
    "            <...>\n",
    "            case 'newFilterName':\n",
    "               apply_newFilterName(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos\n",
    "**FD-Tab demonstation**\n",
    "<div>\n",
    "<iframe width=\"900\" height=\"506\" src=\"https://youtube.com/embed/5wzBoQGOEZA\"></iframe>\n",
    "</div>\n",
    "\n",
    "[Youtube-Link](https://www.youtube.com/watch?v=5wzBoQGOEZA)\n",
    "\n",
    "\n",
    "**FR-Tab demonstration**\n",
    "<div>\n",
    "<iframe width=\"900\" height=\"506\" src=\"https://youtube.com/embed/rygrHl7ffP4\"></iframe>\n",
    "</div>\n",
    "\n",
    "[Youtube-Link](https://www.youtube.com/watch?v=rygrHl7ffP4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Implemented image manipulation ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_dithering`\n",
    "The dithering process in this filter reduces the color range of the image to a predefined palette, in this case, 16 colors. It achieves this through a quantization process that groups similar colors, followed by a conversion back to an RGB format. The outcome is an image characterized by distinct color blocks and a notable reduction in color gradation. This effect can be applied globally to the image or localized to a region determined by facial keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.028126700Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_dithering(image: Image, keypoints, only_face=True) -> Image:\n",
    "    modified_image = image.quantize(colors=16).convert('RGB')\n",
    "    if only_face:\n",
    "        return swap_images_at_face_position(image, keypoints, modified_image)\n",
    "    else:\n",
    "        return modified_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_max_filter`\n",
    "This filter operates by scanning the image and replacing each pixel with the maximum pixel value in its neighborhood, defined by a specified filter size. As a result, brighter areas within the filter's radius become more prominent, while darker regions are subdued. This enhances the luminance contrast and can accentuate certain features of the image. The filter can be applied to the entire image or restricted to a region identified by facial keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.029124100Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_max_filter(image: Image, keypoints, only_face=True) -> Image:\n",
    "    modified_image = image.filter(ImageFilter.MaxFilter(9))\n",
    "    if only_face:\n",
    "        return swap_images_at_face_position(image, keypoints, modified_image)\n",
    "    else:\n",
    "        return modified_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_min_filter`\n",
    "This filter is the inverse of the Max Filter. It scans the image and replaces each pixel with the minimum pixel value in its neighborhood. This process emphasizes darker areas and reduces the prominence of brighter ones. The Min Filter accentuates shadows and darker regions, providing a contrasting effect to the Max Filter. It can be used across the whole image or targeted to a specific area using facial keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.031119200Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_min_filter\u001b[39m(image: \u001b[43mImage\u001b[49m, keypoints, only_face\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image:\n\u001b[0;32m      2\u001b[0m     modified_image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mfilter(ImageFilter\u001b[38;5;241m.\u001b[39mMinFilter(\u001b[38;5;241m9\u001b[39m))\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m only_face:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "def apply_min_filter(image: Image, keypoints, only_face=True) -> Image:\n",
    "    modified_image = image.filter(ImageFilter.MinFilter(9))\n",
    "    if only_face:\n",
    "        return swap_images_at_face_position(image, keypoints, modified_image)\n",
    "    else:\n",
    "        return modified_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_closing`\n",
    "The Closing Filter combines the effects of the Min and Max Filters in sequence. Initially, it applies the Min Filter, which reduces the prominence of smaller bright areas, followed by the Max Filter, which enhances the surrounding brighter regions. This sequence effectively 'closes' small gaps and dark spots, resulting in a smoother appearance in bright areas of the image. This filter can be applied universally or selectively based on keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.047076400Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_closing(image: Image, keypoints, only_face=True) -> Image:\n",
    "    modified_image = apply_min_filter(apply_max_filter(image, keypoints, False), keypoints, False)\n",
    "    if only_face:\n",
    "        return swap_images_at_face_position(image, keypoints, modified_image)\n",
    "    else:\n",
    "        return modified_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_opening`\n",
    "The Opening Filter reverses the sequence of the Closing Filter. It starts with the Max Filter, which diminishes small dark regions, followed by the Min Filter, which then reduces the surrounding darker areas. This 'opening' effect is particularly noticeable in darker parts of the image, creating a sense of expansion in these regions. The filter can be applied to the entire image or localized to a specific area using keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.050068700Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_opening(image: Image, keypoints, only_face=True) -> Image:\n",
    "    modified_image = apply_max_filter(apply_min_filter(image, keypoints, False), keypoints, False)\n",
    "    if only_face:\n",
    "        return swap_images_at_face_position(image, keypoints, modified_image)\n",
    "    else:\n",
    "        return modified_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_color_shift`\n",
    "This filter introduces a random shift in the color channels of the image. It independently alters the red, green, and blue channels by a random value within a specified range. The effect of this is a shift in the overall color balance of the image, producing a variety of color tones and hues. The degree of color shift is controlled by the max_shift_intensity parameter. The filter can modify the entire image or be confined to a particular region, such as the face, based on keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.052062800Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_color_shift(image: Image, keypoints, only_face=True, max_shift_intensity=25) -> Image:\n",
    "    r_shift = random.randint(-max_shift_intensity, max_shift_intensity)\n",
    "    g_shift = random.randint(-max_shift_intensity, max_shift_intensity)\n",
    "    b_shift = random.randint(-max_shift_intensity, max_shift_intensity)\n",
    "    r, g, b = image.split()\n",
    "    r = r.point(lambda i: i + r_shift)\n",
    "    g = g.point(lambda i: i + g_shift)\n",
    "    b = b.point(lambda i: i + b_shift)\n",
    "    modified_image = Image.merge('RGB', (r, g, b))\n",
    "    if only_face:\n",
    "        return swap_images_at_face_position(image, keypoints, modified_image)\n",
    "    else:\n",
    "        return modified_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_whole_face_mask`\n",
    "This filter applies a face mask image over the entire face. It uses facial keypoints to determine the position and orientation of the face. The mask is resized to match the width of the face and rotated to align with the angle between the eyes. The mask is then placed over the face, covering it entirely, simulating the effect of wearing a full-face mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.054059100Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_whole_face_mask(image: Image, keypoints) -> Image:\n",
    "    foreground = Image.open('filters/whole_face_mask.png').convert(\"RGBA\")\n",
    "    for (box, face_keypoints, face_shape_landmarks, _) in keypoints:\n",
    "        left_eye = face_keypoints['left_eye']\n",
    "        right_eye = face_keypoints['right_eye']\n",
    "        dx = right_eye[0] - left_eye[0]\n",
    "        dy = right_eye[1] - left_eye[1]\n",
    "        angle_radians = math.atan2(-dy, dx)\n",
    "        angle_degrees = math.degrees(angle_radians)\n",
    "        face_width = box[2]\n",
    "        foreground_width_to_height_ratio = foreground.size[0] / foreground.size[1]\n",
    "        foreground = foreground.resize(size=(face_width, int(face_width / foreground_width_to_height_ratio)))\n",
    "        rotated_overlay = foreground.rotate(angle_degrees, expand=True)\n",
    "        left_upper_face_mask = (box[0], box[1])\n",
    "        left_upper_paste = (left_upper_face_mask[0], int(left_upper_face_mask[1] - math.fabs(\n",
    "            math.cos(math.radians(90 - angle_degrees)) * face_width)))\n",
    "        image.paste(rotated_overlay, left_upper_paste, rotated_overlay)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_highlight_keypoints`\n",
    "This filter visually highlights the facial keypoints and connections between them on the image. It draws circles around each keypoint and lines connecting them, using different colors for different facial features. This filter serves to visually emphasize the positions and relationships of facial features as identified by the keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.056052300Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_highlight_keypoints(image: Image, keypoints) -> Image:\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    if len(keypoints) > 0:\n",
    "        for keypoint_set in keypoints:\n",
    "            for j in range(len(keypoint_set[2])):\n",
    "                x, y = keypoint_set[2][j]\n",
    "                if j < len(keypoint_set[2]) - 1:\n",
    "                    next_x, next_y = keypoint_set[2][j + 1]\n",
    "                    draw.line((x, y, next_x, next_y), fill='lightgreen', width=3)\n",
    "                radius = 5\n",
    "                draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill='green', outline='lightgreen')\n",
    "            for feature, coords in keypoint_set[1].items():\n",
    "                x, y = coords\n",
    "                radius = 10\n",
    "                draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill='red', outline='red')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_distance_transformation`\n",
    "This filter applies a distance transformation technique to an image. It starts by converting the image to grayscale and then to a binary format based on a specified threshold (in this case, 128). Binary dilation and erosion operations are performed on the binary image to highlight regions of change. Dilation expands the white areas, while erosion shrinks them. The filter then compares the dilated and eroded images, highlighting the differences with white pixels. The result is an image that emphasizes the structural changes in the original image, providing a unique visual representation of distance transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.058047400Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_distance_transformation\u001b[39m(image: \u001b[43mImage\u001b[49m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image:\n\u001b[0;32m      2\u001b[0m     image_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "def apply_distance_transformation(image: Image) -> Image:\n",
    "    image_np = np.array(image.convert('L'))\n",
    "    threshold = 128\n",
    "    binary_image = (image_np > threshold).astype(np.uint8)\n",
    "    dilated = binary_dilation(binary_image, iterations=5)\n",
    "    eroded = binary_erosion(binary_image, iterations=5)\n",
    "    morphed_image = np.where(dilated != eroded, 255, 0).astype(np.uint8)\n",
    "    return Image.fromarray(morphed_image).convert('RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_vertical_edge`\n",
    " This filter emphasizes vertical edges in an image using a specific kernel in a convolution process. The kernel used is designed to respond strongly to vertical lines or edges by subtracting the pixel value on the left from the pixel value on the right. This operation enhances vertical features while diminishing horizontal features. The filter is particularly effective in highlighting vertical structures or details in an image, making them more pronounced against the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.078022700Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_vertical_edge(image: Image) -> Image:\n",
    "    return image.filter(ImageFilter.Kernel((3, 3), (-1, 0, 1, -2, 0, 2, -1, 0, 1), 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_horizontal_edge`\n",
    "Similar to the Vertical Edge Filter, this filter is designed to accentuate horizontal edges in an image. It uses a convolution kernel that contrasts pixel values above and below each pixel in the image. By doing so, horizontal lines or edges become more pronounced. This filter is useful for emphasizing horizontal features or details within the image, enhancing their visibility and distinction from the rest of the image elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.080016800Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_horizontal_edge(image: Image) -> Image:\n",
    "    return image.filter(ImageFilter.Kernel((3, 3), (-1, -2, -1, 0, 0, 0, 1, 2, 1), 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First image manipulation ideas (not in web-app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `create_rotate_image`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T09:28:06.119881200Z",
     "start_time": "2024-02-15T09:28:05.974588200Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_rotate_image(angle: int):\n",
    "    def rotate_image(image: np.ndarray):\n",
    "        pilImage = Image.fromarray(image)\n",
    "        pilImage = pilImage.rotate(angle=angle)\n",
    "        return np.array(pilImage)\n",
    "    return rotate_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `flip_image_horizontally`\n",
    "The function takes a NumPy array representing an image as input, converts it to a PIL Image, flips the image horizontally using the transpose() method with the specified transformation (FLIP_LEFT_RIGHT), and then converts the horizontally flipped PIL Image back to a NumPy array before returning the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T09:28:06.201219300Z",
     "start_time": "2024-02-15T09:28:05.985559500Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflip_image_horizontally\u001b[39m(image: \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m      2\u001b[0m     pilImage \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(image)\n\u001b[0;32m      3\u001b[0m     pilImage \u001b[38;5;241m=\u001b[39m pilImage\u001b[38;5;241m.\u001b[39mtranspose(Image\u001b[38;5;241m.\u001b[39mFLIP_LEFT_RIGHT)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def flip_image_horizontally(image: np.ndarray):\n",
    "    pilImage = Image.fromarray(image)\n",
    "    pilImage = pilImage.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    return np.array(pilImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `change_to_grayscale`\n",
    "The function accepts a NumPy array representing an image as input, transforms it into a PIL Image, converts the color image to grayscale using the convert() method with the 'L' mode, and then converts the resulting grayscale PIL Image back to a NumPy array before returning the processed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.016159600Z"
    }
   },
   "outputs": [],
   "source": [
    "def change_to_grayscale(image: np.ndarray):\n",
    "    pilImage = Image.fromarray(image)\n",
    "    pilImage = pilImage.convert('L')\n",
    "    return np.array(pilImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_gauss_noise`\n",
    "The function generates Gaussian noise with a mean of 50 and a standard deviation of 10 using NumPy's random.normal() function. This noise is created to match the shape of the input image. The generated noise is then added to the original image using the OpenCV add() function, resulting in an image with applied Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.021146Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_gauss_noise(image: np.ndarray):\n",
    "    gauss = np.random.normal(50, 10, image.shape).astype('uint8')\n",
    "    return cv2.add(image, gauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `blur_edges`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.022143300Z"
    }
   },
   "outputs": [],
   "source": [
    "def blur_edges(image: np.ndarray):\n",
    "    # edge detection\n",
    "    grey_scale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    horizontalEdgeImage = convolve2d(grey_scale_image, np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]), mode='same', boundary='symm')\n",
    "    verticalEdgeImage = convolve2d(grey_scale_image, np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]), mode='same', boundary='symm')\n",
    "    plt.matshow(horizontalEdgeImage, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.matshow(verticalEdgeImage, cmap='gray')\n",
    "    plt.show()\n",
    "    edge_image = pow(pow(horizontalEdgeImage, 2) + pow(verticalEdgeImage, 2), 0.5)\n",
    "    plt.matshow(edge_image, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    plt.matshow(np.abs(edge_image), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    print(edge_image.shape)\n",
    "    edge_image_all_colors = np.tile(edge_image, (1, 3))\n",
    "\n",
    "    print(edge_image_all_colors.shape)\n",
    "    edge_image_all_colors_reshaped = edge_image_all_colors.reshape(edge_image.shape[0], edge_image.shape[1], 3)\n",
    "\n",
    "    print(edge_image_all_colors_reshaped.shape)\n",
    "    image_edge_blur = image + ((edge_image_all_colors_reshaped - image) * 0.2).astype('uint8')\n",
    "    plt.matshow(image_edge_blur)\n",
    "    plt.show()\n",
    "\n",
    "    laplacian = cv2.Laplacian(grey_scale_image, cv2.CV_64F)\n",
    "    plt.matshow(laplacian, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    plt.matshow(np.abs(laplacian), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    return image_edge_blur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `create_add_black_squares`\n",
    "The outer function, `create_add_black_squares`, takes two integer parameters, `square_size` and `number_of_squares`, and returns an inner function `add_black_squares`. When this inner function is called with a NumPy array representing an image, it iteratively adds black squares to the image. The size and number of squares are determined by the parameters provided during the creation of the outer function. The positions of the black squares are randomly generated within the image dimensions, and the pixel values within the specified square regions are set to 0 (black). The resulting image is then adjusted to ensure pixel values remain within the valid 0-255 range before being returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.024137100Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_add_black_squares(square_size: int, number_of_squares: int):\n",
    "    def add_black_squares(image: np.ndarray):\n",
    "        for _ in range(number_of_squares):\n",
    "            x = np.random.randint(0, image.shape[0] - square_size)\n",
    "            y = np.random.randint(0, image.shape[1] - square_size)\n",
    "            image[x:x + square_size, y:y + square_size] = 0  # 0 because squares are black\n",
    "\n",
    "        # Ensure the image values stay within 0-255 range\n",
    "        return np.clip(image, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return add_black_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_bilateral_filter`\n",
    "The function applies a bilateral filter to the input image using the OpenCV `bilateralFilter` function. The bilateral filter is a non-linear, edge-preserving smoothing filter that considers both spatial and intensity differences between pixels. The parameters used for this filter are set to 9 for the diameter of the pixel neighborhood, and 75 for both the color and spatial sigma values. These parameters control the extent of filtering in terms of pixel proximity and color similarity. The resulting filtered image is then returned as the output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.026132600Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_bilateral_filter(image):\n",
    "    # bilateral filter (explain params)\n",
    "    filtered_image = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "    return filtered_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
