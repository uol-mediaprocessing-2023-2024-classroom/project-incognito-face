{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First image manipulation ideas (not in web-app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `create_rotate_image`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T09:28:06.119881200Z",
     "start_time": "2024-02-15T09:28:05.974588200Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_rotate_image(angle: int):\n",
    "    def rotate_image(image: np.ndarray):\n",
    "        pilImage = Image.fromarray(image)\n",
    "        pilImage = pilImage.rotate(angle=angle)\n",
    "        return np.array(pilImage)\n",
    "    return rotate_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `flip_image_horizontally`\n",
    "The function takes a NumPy array representing an image as input, converts it to a PIL Image, flips the image horizontally using the transpose() method with the specified transformation (FLIP_LEFT_RIGHT), and then converts the horizontally flipped PIL Image back to a NumPy array before returning the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T09:28:06.201219300Z",
     "start_time": "2024-02-15T09:28:05.985559500Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mflip_image_horizontally\u001B[39m(image: \u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[0;32m      2\u001B[0m     pilImage \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(image)\n\u001B[0;32m      3\u001B[0m     pilImage \u001B[38;5;241m=\u001B[39m pilImage\u001B[38;5;241m.\u001B[39mtranspose(Image\u001B[38;5;241m.\u001B[39mFLIP_LEFT_RIGHT)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def flip_image_horizontally(image: np.ndarray):\n",
    "    pilImage = Image.fromarray(image)\n",
    "    pilImage = pilImage.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    return np.array(pilImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `change_to_grayscale`\n",
    "The function accepts a NumPy array representing an image as input, transforms it into a PIL Image, converts the color image to grayscale using the convert() method with the 'L' mode, and then converts the resulting grayscale PIL Image back to a NumPy array before returning the processed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.016159600Z"
    }
   },
   "outputs": [],
   "source": [
    "def change_to_grayscale(image: np.ndarray):\n",
    "    pilImage = Image.fromarray(image)\n",
    "    pilImage = pilImage.convert('L')\n",
    "    return np.array(pilImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_gauss_noise`\n",
    "The function generates Gaussian noise with a mean of 50 and a standard deviation of 10 using NumPy's random.normal() function. This noise is created to match the shape of the input image. The generated noise is then added to the original image using the OpenCV add() function, resulting in an image with applied Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.021146Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_gauss_noise(image: np.ndarray):\n",
    "    gauss = np.random.normal(50, 10, image.shape).astype('uint8')\n",
    "    return cv2.add(image, gauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `blur_edges`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.022143300Z"
    }
   },
   "outputs": [],
   "source": [
    "def blur_edges(image: np.ndarray):\n",
    "    # edge detection\n",
    "    grey_scale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    horizontalEdgeImage = convolve2d(grey_scale_image, np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]), mode='same', boundary='symm')\n",
    "    verticalEdgeImage = convolve2d(grey_scale_image, np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]), mode='same', boundary='symm')\n",
    "    plt.matshow(horizontalEdgeImage, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.matshow(verticalEdgeImage, cmap='gray')\n",
    "    plt.show()\n",
    "    edge_image = pow(pow(horizontalEdgeImage, 2) + pow(verticalEdgeImage, 2), 0.5)\n",
    "    plt.matshow(edge_image, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    plt.matshow(np.abs(edge_image), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    print(edge_image.shape)\n",
    "    edge_image_all_colors = np.tile(edge_image, (1, 3))\n",
    "\n",
    "    print(edge_image_all_colors.shape)\n",
    "    edge_image_all_colors_reshaped = edge_image_all_colors.reshape(edge_image.shape[0], edge_image.shape[1], 3)\n",
    "\n",
    "    print(edge_image_all_colors_reshaped.shape)\n",
    "    image_edge_blur = image + ((edge_image_all_colors_reshaped - image) * 0.2).astype('uint8')\n",
    "    plt.matshow(image_edge_blur)\n",
    "    plt.show()\n",
    "\n",
    "    laplacian = cv2.Laplacian(grey_scale_image, cv2.CV_64F)\n",
    "    plt.matshow(laplacian, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    plt.matshow(np.abs(laplacian), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    return image_edge_blur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `create_add_black_squares`\n",
    "The outer function, `create_add_black_squares`, takes two integer parameters, `square_size` and `number_of_squares`, and returns an inner function `add_black_squares`. When this inner function is called with a NumPy array representing an image, it iteratively adds black squares to the image. The size and number of squares are determined by the parameters provided during the creation of the outer function. The positions of the black squares are randomly generated within the image dimensions, and the pixel values within the specified square regions are set to 0 (black). The resulting image is then adjusted to ensure pixel values remain within the valid 0-255 range before being returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.024137100Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_add_black_squares(square_size: int, number_of_squares: int):\n",
    "    def add_black_squares(image: np.ndarray):\n",
    "        for _ in range(number_of_squares):\n",
    "            x = np.random.randint(0, image.shape[0] - square_size)\n",
    "            y = np.random.randint(0, image.shape[1] - square_size)\n",
    "            image[x:x + square_size, y:y + square_size] = 0  # 0 because squares are black\n",
    "\n",
    "        # Ensure the image values stay within 0-255 range\n",
    "        return np.clip(image, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return add_black_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_bilateral_filter`\n",
    "The function applies a bilateral filter to the input image using the OpenCV `bilateralFilter` function. The bilateral filter is a non-linear, edge-preserving smoothing filter that considers both spatial and intensity differences between pixels. The parameters used for this filter are set to 9 for the diameter of the pixel neighborhood, and 75 for both the color and spatial sigma values. These parameters control the extent of filtering in terms of pixel proximity and color similarity. The resulting filtered image is then returned as the output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.026132600Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_bilateral_filter(image):\n",
    "    # bilateral filter (explain params)\n",
    "    filtered_image = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "    return filtered_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Implemented image manipulation ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_dithering`\n",
    "The dithering process in this filter reduces the color range of the image to a predefined palette, in this case, 16 colors. It achieves this through a quantization process that groups similar colors, followed by a conversion back to an RGB format. The outcome is an image characterized by distinct color blocks and a notable reduction in color gradation. This effect can be applied globally to the image or localized to a region determined by facial keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.028126700Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_dithering(image: Image, keypoints, only_face=True) -> Image:\n",
    "    modified_image = image.quantize(colors=16).convert('RGB')\n",
    "    if only_face:\n",
    "        return swap_images_at_face_position(image, keypoints, modified_image)\n",
    "    else:\n",
    "        return modified_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_max_filter`\n",
    "This filter operates by scanning the image and replacing each pixel with the maximum pixel value in its neighborhood, defined by a specified filter size. As a result, brighter areas within the filter's radius become more prominent, while darker regions are subdued. This enhances the luminance contrast and can accentuate certain features of the image. The filter can be applied to the entire image or restricted to a region identified by facial keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.029124100Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_max_filter(image: Image, keypoints, only_face=True) -> Image:\n",
    "    modified_image = image.filter(ImageFilter.MaxFilter(9))\n",
    "    if only_face:\n",
    "        return swap_images_at_face_position(image, keypoints, modified_image)\n",
    "    else:\n",
    "        return modified_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_min_filter`\n",
    "This filter is the inverse of the Max Filter. It scans the image and replaces each pixel with the minimum pixel value in its neighborhood. This process emphasizes darker areas and reduces the prominence of brighter ones. The Min Filter accentuates shadows and darker regions, providing a contrasting effect to the Max Filter. It can be used across the whole image or targeted to a specific area using facial keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.031119200Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_min_filter\u001B[39m(image: \u001B[43mImage\u001B[49m, keypoints, only_face\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Image:\n\u001B[0;32m      2\u001B[0m     modified_image \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mfilter(ImageFilter\u001B[38;5;241m.\u001B[39mMinFilter(\u001B[38;5;241m9\u001B[39m))\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m only_face:\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "def apply_min_filter(image: Image, keypoints, only_face=True) -> Image:\n",
    "    modified_image = image.filter(ImageFilter.MinFilter(9))\n",
    "    if only_face:\n",
    "        return swap_images_at_face_position(image, keypoints, modified_image)\n",
    "    else:\n",
    "        return modified_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_closing`\n",
    "The Closing Filter combines the effects of the Min and Max Filters in sequence. Initially, it applies the Min Filter, which reduces the prominence of smaller bright areas, followed by the Max Filter, which enhances the surrounding brighter regions. This sequence effectively 'closes' small gaps and dark spots, resulting in a smoother appearance in bright areas of the image. This filter can be applied universally or selectively based on keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.047076400Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_closing(image: Image, keypoints, only_face=True) -> Image:\n",
    "    modified_image = apply_min_filter(apply_max_filter(image, keypoints, False), keypoints, False)\n",
    "    if only_face:\n",
    "        return swap_images_at_face_position(image, keypoints, modified_image)\n",
    "    else:\n",
    "        return modified_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_opening`\n",
    "The Opening Filter reverses the sequence of the Closing Filter. It starts with the Max Filter, which diminishes small dark regions, followed by the Min Filter, which then reduces the surrounding darker areas. This 'opening' effect is particularly noticeable in darker parts of the image, creating a sense of expansion in these regions. The filter can be applied to the entire image or localized to a specific area using keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.050068700Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_opening(image: Image, keypoints, only_face=True) -> Image:\n",
    "    modified_image = apply_max_filter(apply_min_filter(image, keypoints, False), keypoints, False)\n",
    "    if only_face:\n",
    "        return swap_images_at_face_position(image, keypoints, modified_image)\n",
    "    else:\n",
    "        return modified_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_color_shift`\n",
    "This filter introduces a random shift in the color channels of the image. It independently alters the red, green, and blue channels by a random value within a specified range. The effect of this is a shift in the overall color balance of the image, producing a variety of color tones and hues. The degree of color shift is controlled by the max_shift_intensity parameter. The filter can modify the entire image or be confined to a particular region, such as the face, based on keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.052062800Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_color_shift(image: Image, keypoints, only_face=True, max_shift_intensity=25) -> Image:\n",
    "    r_shift = random.randint(-max_shift_intensity, max_shift_intensity)\n",
    "    g_shift = random.randint(-max_shift_intensity, max_shift_intensity)\n",
    "    b_shift = random.randint(-max_shift_intensity, max_shift_intensity)\n",
    "    r, g, b = image.split()\n",
    "    r = r.point(lambda i: i + r_shift)\n",
    "    g = g.point(lambda i: i + g_shift)\n",
    "    b = b.point(lambda i: i + b_shift)\n",
    "    modified_image = Image.merge('RGB', (r, g, b))\n",
    "    if only_face:\n",
    "        return swap_images_at_face_position(image, keypoints, modified_image)\n",
    "    else:\n",
    "        return modified_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_whole_face_mask`\n",
    "This filter applies a face mask image over the entire face. It uses facial keypoints to determine the position and orientation of the face. The mask is resized to match the width of the face and rotated to align with the angle between the eyes. The mask is then placed over the face, covering it entirely, simulating the effect of wearing a full-face mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.054059100Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_whole_face_mask(image: Image, keypoints) -> Image:\n",
    "    foreground = Image.open('filters/whole_face_mask.png').convert(\"RGBA\")\n",
    "    for (box, face_keypoints, face_shape_landmarks, _) in keypoints:\n",
    "        left_eye = face_keypoints['left_eye']\n",
    "        right_eye = face_keypoints['right_eye']\n",
    "        dx = right_eye[0] - left_eye[0]\n",
    "        dy = right_eye[1] - left_eye[1]\n",
    "        angle_radians = math.atan2(-dy, dx)\n",
    "        angle_degrees = math.degrees(angle_radians)\n",
    "        face_width = box[2]\n",
    "        foreground_width_to_height_ratio = foreground.size[0] / foreground.size[1]\n",
    "        foreground = foreground.resize(size=(face_width, int(face_width / foreground_width_to_height_ratio)))\n",
    "        rotated_overlay = foreground.rotate(angle_degrees, expand=True)\n",
    "        left_upper_face_mask = (box[0], box[1])\n",
    "        left_upper_paste = (left_upper_face_mask[0], int(left_upper_face_mask[1] - math.fabs(\n",
    "            math.cos(math.radians(90 - angle_degrees)) * face_width)))\n",
    "        image.paste(rotated_overlay, left_upper_paste, rotated_overlay)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_highlight_keypoints`\n",
    "This filter visually highlights the facial keypoints and connections between them on the image. It draws circles around each keypoint and lines connecting them, using different colors for different facial features. This filter serves to visually emphasize the positions and relationships of facial features as identified by the keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.056052300Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_highlight_keypoints(image: Image, keypoints) -> Image:\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    if len(keypoints) > 0:\n",
    "        for keypoint_set in keypoints:\n",
    "            for j in range(len(keypoint_set[2])):\n",
    "                x, y = keypoint_set[2][j]\n",
    "                if j < len(keypoint_set[2]) - 1:\n",
    "                    next_x, next_y = keypoint_set[2][j + 1]\n",
    "                    draw.line((x, y, next_x, next_y), fill='lightgreen', width=3)\n",
    "                radius = 5\n",
    "                draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill='green', outline='lightgreen')\n",
    "            for feature, coords in keypoint_set[1].items():\n",
    "                x, y = coords\n",
    "                radius = 10\n",
    "                draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill='red', outline='red')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_distance_transformation`\n",
    "This filter applies a distance transformation technique to an image. It starts by converting the image to grayscale and then to a binary format based on a specified threshold (in this case, 128). Binary dilation and erosion operations are performed on the binary image to highlight regions of change. Dilation expands the white areas, while erosion shrinks them. The filter then compares the dilated and eroded images, highlighting the differences with white pixels. The result is an image that emphasizes the structural changes in the original image, providing a unique visual representation of distance transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.058047400Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_distance_transformation\u001B[39m(image: \u001B[43mImage\u001B[49m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Image:\n\u001B[0;32m      2\u001B[0m     image_np \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(image\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m      3\u001B[0m     threshold \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m128\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "def apply_distance_transformation(image: Image) -> Image:\n",
    "    image_np = np.array(image.convert('L'))\n",
    "    threshold = 128\n",
    "    binary_image = (image_np > threshold).astype(np.uint8)\n",
    "    dilated = binary_dilation(binary_image, iterations=5)\n",
    "    eroded = binary_erosion(binary_image, iterations=5)\n",
    "    morphed_image = np.where(dilated != eroded, 255, 0).astype(np.uint8)\n",
    "    return Image.fromarray(morphed_image).convert('RGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_vertical_edge`\n",
    " This filter emphasizes vertical edges in an image using a specific kernel in a convolution process. The kernel used is designed to respond strongly to vertical lines or edges by subtracting the pixel value on the left from the pixel value on the right. This operation enhances vertical features while diminishing horizontal features. The filter is particularly effective in highlighting vertical structures or details in an image, making them more pronounced against the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.078022700Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_vertical_edge(image: Image) -> Image:\n",
    "    return image.filter(ImageFilter.Kernel((3, 3), (-1, 0, 1, -2, 0, 2, -1, 0, 1), 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply_horizontal_edge`\n",
    "Similar to the Vertical Edge Filter, this filter is designed to accentuate horizontal edges in an image. It uses a convolution kernel that contrasts pixel values above and below each pixel in the image. By doing so, horizontal lines or edges become more pronounced. This filter is useful for emphasizing horizontal features or details within the image, enhancing their visibility and distinction from the rest of the image elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T09:28:06.080016800Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_horizontal_edge(image: Image) -> Image:\n",
    "    return image.filter(ImageFilter.Kernel((3, 3), (-1, -2, -1, 0, 0, 0, 1, 2, 1), 1, 0))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
