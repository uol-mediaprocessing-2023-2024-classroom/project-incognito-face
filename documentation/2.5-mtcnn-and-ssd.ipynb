{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTCNN face detection\n",
    "\n",
    "A more recently used approach is the MTCNN face detection method. MTCNN stands for Multi-task Cascaded Convolutional Networks and, as the name suggests, is based on Convolution Neural Networks. As the algorithm, like Viola-Jones, has a cascaded structure and can therefore exclude non-face regions at an early stage, the method is suitable for real-time face detection. MTCNN basically consists of 3 different steps:\n",
    "1. Face Detection\n",
    "2. Facial Landmark Detection\n",
    "3. Face Classification\n",
    "\n",
    "In the following, we will give a brief overview of the steps in MTCNN.\n",
    "\n",
    "### MTCNN Steps\n",
    "\n",
    "#### Step 1 - Face Detection\n",
    "\n",
    "In the first step, the MTCNN recognizes potential candidate faces in the input image. It uses a cascade of convolutional networks to filter out regions that are unlikely to contain a face and focuses on regions with a higher probability of containing a face.\n",
    "The cascade structure comprises several stages consisting of different CNNs. At each stage, the network limits the number of eligible regions by the result of the CNN.\n",
    "The end result of this step is a series of bounding boxes that represent the potential face regions in the image.\n",
    "\n",
    "#### Step 2 - Facial Landmark Detection\n",
    "Once the potential facial regions are identified, the second step of MTCNN is responsible for locating facial keypoints within each bounding box.\n",
    "Facial keypoints are specific points on the face, such as the eyes, nose and mouth. These landmarks are critical for tasks such as facial alignment and detection.\n",
    "The network at this step is designed to regress the coordinates of these facial features for each recognized face.\n",
    "\n",
    "#### Step 3 - Face Classification\n",
    "\n",
    "The third step of MTCNN deals with the classification of each bounding box as face or non-face. This step helps to eliminate false positives and improves the accuracy of the overall face recognition system.\n",
    "A classifier is trained to distinguish between faces and non-faces by extracting features from the candidate regions. \n",
    "The result of this step is a refined set of bounding boxes, with the corresponding face keypoints, which are more likely to contain actual faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Application\n",
    "\n",
    "Our python implementation for MTCNN is using the following Code in the backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T15:27:15.191308300Z",
     "start_time": "2023-12-22T15:27:15.136428200Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhighlight_face_mtcnn\u001B[39m(img: \u001B[43mImage\u001B[49m):\n\u001B[0;32m      2\u001B[0m     img \u001B[38;5;241m=\u001B[39m numpy\u001B[38;5;241m.\u001B[39marray(img)\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;66;03m# Disable printing\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "def highlight_face_mtcnn(img: Image):\n",
    "    img = numpy.array(img)\n",
    "    # Disable printing\n",
    "    with io.StringIO() as dummy_stdout:\n",
    "        with redirect_stdout(dummy_stdout):\n",
    "            faces = mtcnn_detector.detect_faces(img)\n",
    "\n",
    "    confidence = 100\n",
    "\n",
    "    for face in faces:\n",
    "        confidence = round(face['confidence'] * 100, 3)\n",
    "        x, y, w, h = face['box'][0], face['box'][1], face['box'][2], face['box'][3]\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 255), 6)\n",
    "\n",
    "    return Image.fromarray(img), len(faces), confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation of MTCNN face detection processes a given PIL Image object. It converts the image to a numpy array. Using a pre-trained mtcnn detector, it detects faces in the image. This pre-trained detector is already provided by the library, so we do not need to train or create our own. Detected faces are outlined with pink rectangles, and are then being returned.\n",
    "\n",
    "Here is an example output of the algorithm:\n",
    "\n",
    "![Example of a detected face](images/detected-faces-examples/detected_face_mtcnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SSD face detection\n",
    "\n",
    "TODO!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Practical Application\n",
    "\n",
    "Our python implementation for SSD is using the following Code in the backend:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def highlight_face_ssd(img: Image):\n",
    "    img = cv2.cvtColor(numpy.array(img), cv2.COLOR_RGB2BGR)\n",
    "    resized_rgb_image = cv2.resize(img, (300, 300))\n",
    "    imageBlob = cv2.dnn.blobFromImage(image=resized_rgb_image)\n",
    "    ssd_detector.setInput(imageBlob)\n",
    "    detections = ssd_detector.forward()\n",
    "\n",
    "    confidence = 100\n",
    "    number_of_faces = 0\n",
    "\n",
    "    # only show detections over 80% certainty\n",
    "    for row in detections[0][0]:\n",
    "        if row[2] > 0.80:\n",
    "            confidence = round(row[2] * 100, 3)\n",
    "            number_of_faces += 1\n",
    "            x1, y1, x2, y2 = int(row[3] * img.shape[1]), int(row[4] * img.shape[0]), int(row[5] * img.shape[1]), int(\n",
    "                row[6] * img.shape[0])\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 255), 6)\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(img_rgb), number_of_faces, confidence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-22T15:27:15.165350500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This implementation of SSD face detection processes a given PIL Image object. It converts the image to BGR format and then resized to 300x300 pixels. Using a pre-trained ssd detector, it detects faces in the image. This pre-trained detector is already provided by the library, so we do not need to train or create our own. The detector provides a result for each sliding window position, so we have to sort out unwanted results. Detected faces are outlined with yellow rectangles, and are then being returned.\n",
    "\n",
    "Here is an example output of the algorithm:\n",
    "\n",
    "![Example of a detected face](images/detected-faces-examples/detected_face_ssd.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
