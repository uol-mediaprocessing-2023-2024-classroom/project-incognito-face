{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Face Recognition\n",
    "\n",
    "After we had completed the part with face detection, we wanted to deal with the area of face recognition. Face recognition is simpler than face detection, as a face must first be detected before it can be recognised. But what is face recognition?\n",
    "Face recognition is the task of recognising faces. However, recognising faces can mean several things. On the one hand, it can mean that faces can be assigned to known persons. These known persons are then stored in a database and for each picture of a face it can then be said whether it is, for example, Olaf Scholz or Christian Lindner. Another method of recognition is that you don't even know which person a face belongs to, but you can say that two faces belong to the same person.\n",
    "\n",
    "![Face Recognition with Database](images/FaceRecognitionDatabase.png)\n",
    "\n",
    "As we don't want to create a large database that might have to store sensitive data, we decided that our WebApp should only be able to tell whether two pictures show the same person.\n",
    "To do this, we used the [face-recognition library](https://github.com/ageitgey/face_recognition) . This library offers many useful methods for face recognition. The library uses the face recognition model from [dlib](http://dlib.net/files/dlib_face_recognition_resnet_model_v1.dat.bz2) to compare the faces. The model calculates a 128D vector from the keypoints of the face, which represents the characteristics of the face. Two of these vectors can then be checked for similarity using the Euclidean distance. If the distance is below a defined threshold value, it is assumed that the person is the same. The model has achieved an accuracy of 99.38% on the standard LFW face recognition benchmark. Dlib also provides a small [demo](http://dlib.net/face_recognition.py.html) if you want to test the model yourself.\n",
    "\n",
    "The face-recognition library provides two different methods for face recognition. The first method is based on five keypoints, taking only two keypoints from both eyes and one keypoint from the nose. The other method is based on 68 keypoints and uses the same keypoints that we use for the filters. As the difference in speed is very small, but the accuracy of 68 keypoints was better, we decided in favour of the 68 keypoints approach.\n",
    "\n",
    "The whole process is shown in this diagram:\n",
    "\n",
    "![Process of face recognition](images/FaceRecognitionProcess.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49513190f916b10b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Practical Application\n",
    "\n",
    "Our python implementation for the face recognition is using the following Code in the backend:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f1ca47fbdfa263d"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (341758604.py, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[8], line 41\u001B[1;36m\u001B[0m\n\u001B[1;33m    return Image.fromarray(orig_img_rgb), Image.fromarray(mod_img_rgb), count_of_matches Code\u001B[0m\n\u001B[1;37m                                                                                         ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def recognize_faces(orig_img: Image, mod_img: Image, orig_keypoints):\n",
    "    orig_img_bgr = cv2.cvtColor(np.array(orig_img), cv2.COLOR_RGB2BGR)\n",
    "    mod_img_bgr = cv2.cvtColor(np.array(mod_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    gray_image = cv2.cvtColor(np.asarray(mod_img_bgr), cv2.COLOR_BGR2GRAY)\n",
    "    faces = hog_svm_detector(gray_image)\n",
    "    face_encodings_unknown = []\n",
    "    boxes_mod = []\n",
    "    for face in faces:\n",
    "        box = [face.left(), face.top(), face.width(), face.height()]\n",
    "        face_encodings_unknown.append(np.array(calculate_face_encoding(np.asarray(mod_img_bgr), box)))\n",
    "        boxes_mod.append((face.left(), face.top(), face.width(), face.height()))\n",
    "\n",
    "    face_encodings_orig = []\n",
    "    boxes_orig = []\n",
    "    for box, _, _, face_encoding_orig in orig_keypoints:\n",
    "        face_encodings_orig.append(np.array(face_encoding_orig))\n",
    "        boxes_orig.append(box)\n",
    "\n",
    "    count_of_matches = 0\n",
    "\n",
    "    for j, face_encoding_unknown in enumerate(face_encodings_unknown):\n",
    "        matches = face_recognition.compare_faces(face_encodings_orig, face_encoding_unknown, tolerance=0.55)\n",
    "\n",
    "        for i, match in enumerate(matches):\n",
    "            if match:\n",
    "                count_of_matches += 1\n",
    "                selected_color = palette[(j * 2) % num_colors]\n",
    "                bgr_color = tuple(int(value * 255) for value in selected_color)\n",
    "                box_orig = boxes_orig[i]\n",
    "                box_mod = boxes_mod[j]\n",
    "                top, right, bottom, left = box_orig[1], box_orig[0] + box_orig[2], box_orig[1] + box_orig[3], box_orig[\n",
    "                    0]\n",
    "                cv2.rectangle(orig_img_bgr, (left, top), (right, bottom), bgr_color, 6)\n",
    "\n",
    "                top, right, bottom, left = box_mod[1], box_mod[0] + box_mod[2], box_mod[1] + box_mod[3], box_mod[0]\n",
    "                cv2.rectangle(mod_img_bgr, (left, top), (right, bottom), bgr_color, 6)\n",
    "\n",
    "    orig_img_rgb = cv2.cvtColor(orig_img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    mod_img_rgb = cv2.cvtColor(mod_img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(orig_img_rgb), Image.fromarray(mod_img_rgb), count_of_matches Code"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T11:42:06.151148200Z",
     "start_time": "2024-02-13T11:42:06.087720200Z"
    }
   },
   "id": "289054ec09491e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This implementation takes an original image with the calculated keypoints, which also contains the 128D vector of the characteristics of the face and a potential modified image that is compared to the original image. The modified image is pre-processed and then the face bounding boxes are calculated using the svm+hog face detection algorithm. The bounding boxes are then used to compute the 128D vector of the faces found in the image. \n",
    "\n",
    "Each face found in the modified image is then compared to each face in the original image. If the Euclidean distance is less than the given tolerance of 0.55, a match is registered. For each match, a pair of colored boxes is drawn in both of the images where the matching faces are. The number of pairs is also counted.\n",
    "\n",
    "Here is an example output of the algorithm:\n",
    "\n",
    "![Example of a recognized faces](images/detected-faces-examples/example_face_recognition.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e402ba0cabf69b2a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Modification\n",
    "\n",
    "This part of the documentation will investigate how precise filters can prevent the face recognition. The images, which were used during this section, stem from the Labeled Faces in the Wild (LFW) dataset. The dataset contains more than 13,000 images of 5,749 people. In this section, we only consider pairs of two images containing the same person, resulting in 1100 final image pairs. Without any modifications, the algorithm recognizes 918/1100 faces.\n",
    "\n",
    "### Filters\n",
    "\n",
    "The filters we use are based on those already used for face detection and new filters specifically designed to prevent face recognition. The description of the face detection filters can be found in the face detection section.\n",
    "\n",
    "TODO: Link zu Face Detection Teil einf端gen\n",
    "\n",
    "### Morph Eyes\n",
    "\n",
    "TODO: Beschreibung von Robin einf端gen\n",
    "\n",
    "![Morph Eyes with morph strength of 5](images/MorphEyeswithmorphstrengthof5.png)\n",
    "\n",
    "### Morph Mouth\n",
    "\n",
    "TODO: Beschreibung von Robin einf端gen\n",
    "\n",
    "![Morph Mouth with morph strength of 5](images/MorphMouthwithmorphstrengthof5.png)\n",
    "\n",
    "### Morph All\n",
    "\n",
    "TODO: Beschreibung von Robin einf端gen\n",
    "\n",
    "![Morph All with morph strength of 5](images/MorphAllwithmorphstrengthof5.png)\n",
    "\n",
    "### Result\n",
    "\n",
    "We only tested our most promising filters which should prevent face recognition without altering the image too much. The result is shown in the bar chart below:\n",
    "\n",
    "![Result of Face Recognition Analysis](images/FaceRecognitionAnalysis.png)\n",
    "\n",
    "The diagram shows that many of the filters achieve good results.\n",
    "The Salt&Pepper and Cowface filters are probably so good because they prevent face detection. The Medicine Mask filter hides the lower keypoints, which is why it's so good. It is also noticeable that the keypoints of the eyes are very important for face recognition, as Morph Eyes is significantly better than Morph Mouth. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82c0ec8036dd9835"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
