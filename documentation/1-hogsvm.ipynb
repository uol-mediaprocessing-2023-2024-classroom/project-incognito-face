{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f6742ca750b343",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## SVM with HOG features\n",
    "Another approach to detect faces is using a Histogram of Oriented Gradients (HOG) in combination with a support vector machine as classifier.\n",
    "HOG is a feature descriptor and is commonly used in image processing. HOG is an algorithm that typically consists of the following steps:\n",
    "1. Image Preprocessing\n",
    "2. Calculate Gradient\n",
    "3. Create Histogram of Oriented Gradients\n",
    "4. Normalise Histogram Vectors\n",
    "\n",
    "In the following, we will give a brief overview of the steps in HOG.\n",
    "\n",
    "### HOG Steps\n",
    "\n",
    "#### Step 1 - Image Preprocessing\n",
    "\n",
    "To divide the image into blocks later on, the image is first transformed into one with a width/height-ratio of 1:2 (commonly 64x128). Afterwards, the image is divided into blocks (commonly 8x8) of pixels. For the case of an 64x128 image, the resulting image would be a 8x16 grid with 8x8 blocks. In the following, we will assume a block size of 8x8 and an image size of 64x128 to simplify the explanation. \n",
    "\n",
    "#### Step 2 - Calculate Gradient\n",
    "\n",
    "For each of these 8x8 pixels, the gradient vector is calculated. Therefore, we have to calculate the gradient in the x and y-direction. For a given pixel at the position (x,y) on an image I, the gradient in x direction is calculated as follows:\n",
    "$$G_x = I(x+1, y) - I(x-1, y)$$\n",
    "and the gradient in y-direction as:\n",
    "$$G_y = I(x, y+1) - I(x, y-1)$$\n",
    "\n",
    "![Alt text](images/Hog8x8Grid.png)\n",
    "\n",
    "For the pixel 60 on the image, the gradient in x-direction will be:\n",
    "$$G_x = I(x+1, y) - I(x-1, y) = 70 - 40 = 30$$\n",
    "\n",
    "and for the y-direction:\n",
    "$$G_y = I(x, y+1) - I(x, y-1) = 70 - 20 = 50$$\n",
    "\n",
    "Using the gradient in x and y direction, the magnitude and direction of the gradient vector will be calculated using:\n",
    "$$\\text{magnitude} = \\sqrt{G_x^2 + G_y^2}$$\n",
    "$$\\text{direction} = \\arctan\\left(\\frac{G_y}{G_x}\\right)$$\n",
    "\n",
    "For the example the magnitude would be:\n",
    "$$\\sqrt{G_x^2 + G_y^2} = \\sqrt{30^2 + 50^2} \\approx 58.31 $$\n",
    "\n",
    "and the direction would be:\n",
    "$$\\arctan\\left(\\frac{50}{30}\\right) \\approx 59.04 \\degree $$\n",
    "\n",
    "This processed is repeated for every pixel in the 8x8 block so that we receive one 8x8 matrix with the direction of the gradient vector and one 8x8 matrix with the magnitude of the gradient vector for the corresponding pixels.\n",
    "\n",
    "#### Step 3 - Create Histogram of Oriented Gradients\n",
    "\n",
    "In the next step, the Histogram of Oriented Gradients is created with the 8x8 matrix of the direction of the gradient vector and the 8x8 matrix of the magnitude of the gradient vector.\n",
    "\n",
    "![Alt text](images/gradient-histogram.png) TODO: Change the image. Currently the classes are not perfectly represented!\n",
    "\n",
    "Typically, the direction values are normalised to values between 0 and 180 degrees and are used on the x-axis. On the y-axis a running sum of the magnitude values for a given direction is displayed. For a given direction of the gradient vector of a pixel, the corresponding magnitude is added to the bin on the x-axis. For instance, if the gradient vector of a given pixel has a direction of 20 and a magnitude of 60, the value 60 is added to the running sum on the y-axis. If the direction is not exactly in one of these bins, the magnitude has to be split up and placed in the two closest classes. For instance, if the gradient vector of a given pixel has a direction of 30 and a magnitude of 60, then 30 has to be added to the 20 class and 30 has to be added to the 40 class.\n",
    "\n",
    "For each of these 8x8 Blocks, a Histogram of Oriented Gradients with 9 entries will be received. These 9 entries, which are the 9 bins on the x-axis of the histogram, can be used in a 9x1 vector for the next step.\n",
    "\n",
    "#### Step 4 - Normalise Histogram Vectors\n",
    "\n",
    "To enhance the robustness, make the descriptor invariant to changes in illumination, and to improve the classification performance, the resulting 9x1 vectors for each of these 8x8 blocks will have to be normalised. A common way to do that is to use 4 of these 8x8 blocks, which forms one 16x16 block. This 16x16 block consists of four 9x1 vectors, which can be linearised to one 36x1 vector. This vector is then normalised using the following formulas:\n",
    "\n",
    "$$\\text{magnitude} = \\sqrt{b_1^2 + b_2^2 + ..... + b_{36}^2}$$\n",
    "$$\\text{normalised vector} = [\\frac{b_1}{\\text{magnitude}}, \\frac{b_2}{\\text{magnitude}}, ....., \\frac{b_{36}}{\\text{magnitude}}]$$\n",
    "\n",
    "As a result, we receive a normalised 36x1 vector that describes the feature of a 16x16 block. A 64x128 image has 7x15 of these 16x16 blocks, so in total we have 7x15x36x1 = 3780 entries, that are often placed in a 1x3780 vector. This vector probably has to be reduced (to prevent overfitting) using for instance PCA (Principal Component Analysis). However, this will not be explained in this article. We assume that the vector will be fed to an SVM in the following.\n",
    "\n",
    "### SVM with HOG features\n",
    "\n",
    "The resulting vector from the HOG algorithm, that was potentially reduced using PCA, is often fed to an SVM. The SVM tries to find a hyperplane that best separates the datapoints of different classes in a high-dimensional space. On a basic level, the datapoints can be classified into images that contain a face (positive samples), and images that don't contain a face (negative samples). The HOG features extracted from negative and positive samples can then be used to train the SVM so that it learns to distinguish between images that contain faces and ones that don't. Additionally, a trained SVM can be used as a sliding window that analyses a small part of a predefined size of the image to determine whether this part contains a face or not. This allows to not only classify images with faces correctly, but also to detect faces on the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130b5c2d56a355",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Practical Application\n",
    "\n",
    "In order to test an SVM with HOG features in practice, we decide to use the LFW dataset that is built-in sklearn. To start, we imported the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd122f743ec590d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T17:24:34.426446400Z",
     "start_time": "2023-11-14T17:24:34.412931500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "from PIL import Image, ImageFilter\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../test/')\n",
    "import ImageModification as imod\n",
    "\n",
    "def get_faces():\n",
    "    return datasets.fetch_lfw_people(color=True, min_faces_per_person=100, resize=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95504c15a36e935",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Afterwards, we created the function `count_detected_faces_hog` that counts the number of faces that the SVM classifier with HOG features from dlib detected. The parameter `image_mod_fn` is a function that modifies the image before it is fed to the SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e5beae057f3e9da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T17:24:35.973893900Z",
     "start_time": "2023-11-14T17:24:35.960292200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_detected_faces_hog(image_mod_fn: Callable[[np.ndarray], np.ndarray]):\n",
    "    faces = get_faces()\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    counter_faces = 0\n",
    "\n",
    "    for possibleFace in faces.images:\n",
    "        scaled_rgb_image = (possibleFace * 255).astype(np.uint8)\n",
    "\n",
    "        modifiedImage = image_mod_fn(scaled_rgb_image)\n",
    "\n",
    "        detected_faces = detector(modifiedImage)\n",
    "\n",
    "        if len(detected_faces) > 0:\n",
    "            counter_faces += 1\n",
    "\n",
    "    return float(counter_faces) / float(len(faces.images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d0b5d1d28a9701",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then we used several modifications and plotted the success rate of detecting the faces in 1288 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa700d4707a3c0a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def image_modification_plot(count_detected_faces: Callable[[Callable[[np.ndarray], np.ndarray]], float], title: str):\n",
    "    categories = [\"Unmodified\", \"Blur\", \"Rotate 20°\", \"Rotate 90°\", \"Flip Horizontally\", \"Change to Grayscale\"]\n",
    "    values = [count_detected_faces(imod.identity), count_detected_faces(imod.apply_blur),\n",
    "              count_detected_faces(imod.create_rotate_image(20)),\n",
    "              count_detected_faces(imod.create_rotate_image(90)), count_detected_faces(imod.flip_image_horizontally),\n",
    "              count_detected_faces(imod.change_to_grayscale)]\n",
    "\n",
    "    # Create bar chart\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(categories, values, color='skyblue')\n",
    "    plt.ylim(0, 1.0)\n",
    "\n",
    "    # Title and labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Modification Operations')\n",
    "    plt.ylabel('Proportion detected')\n",
    "\n",
    "    # Show plot\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "image_modification_plot(count_detected_faces_hog, \"HOG modifications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0c42a4a65f7e2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The bar chart shows that HOG is not susceptible to a horizontal flip, a conversion to grayscale, and a small rotation, but it is susceptible to blur and a 90° rotation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a713cd0192e18687",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TODO: Schlauere Varianten probieren, die Menschen nicht erkennen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
