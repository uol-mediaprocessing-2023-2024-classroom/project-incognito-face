{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Face Recognition\n",
    "\n",
    "Face recognition is the task of recognising faces. However, recognising faces can mean several things. On the one hand, it can mean that faces can be assigned to known persons. These known persons are then stored in a database and for each picture of a face it can then be said whether it is, for example, Olaf Scholz or Christian Lindner. Another method of recognition is that you don't even know which person a face belongs to, but you can say that two faces belong to the same person.\n",
    "\n",
    "![Face Recognition with Database](images/FaceRecognitionDatabase.png)\n",
    "\n",
    "As we don't want to create a large database that might have to store sensitive data, we decided that our WebApp should only be able to tell whether two pictures show the same person.\n",
    "To do this, we used the [face-recognition library](https://github.com/ageitgey/face_recognition) . This library offers many useful methods for face recognition. The library uses the face recognition model from [dlib](http://dlib.net/files/dlib_face_recognition_resnet_model_v1.dat.bz2) to compare the faces. The model calculates a 128D vector from the keypoints of the face, which represents the characteristics of the face. Two of these vectors can then be checked for similarity using the Euclidean distance. If the distance is below a defined threshold value, it is assumed that the person is the same. The model has achieved an accuracy of 99.38% on the standard LFW face recognition benchmark. Dlib also provides a small [demo](http://dlib.net/face_recognition.py.html) if you want to test the model yourself.\n",
    "\n",
    "The face-recognition library provides two different methods for face recognition. The first method is based on five keypoints, taking only two keypoints from both eyes and one keypoint from the nose. The other method is based on 68 keypoints and uses the same keypoints that we use for the filters. As the difference in speed is very small, but the accuracy of 68 keypoints was better, we decided in favour of the 68 keypoints approach.\n",
    "\n",
    "The whole process is shown in this diagram:\n",
    "\n",
    "![Process of face recognition](images/FaceRecognitionProcess.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49513190f916b10b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Practical Application\n",
    "\n",
    "Our python implementation for the face recognition is using the following Code in the backend:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f1ca47fbdfa263d"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (341758604.py, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[1], line 41\u001B[1;36m\u001B[0m\n\u001B[1;33m    return Image.fromarray(orig_img_rgb), Image.fromarray(mod_img_rgb), count_of_matches Code\u001B[0m\n\u001B[1;37m                                                                                         ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def recognize_faces(orig_img: Image, mod_img: Image, orig_keypoints):\n",
    "    orig_img_bgr = cv2.cvtColor(np.array(orig_img), cv2.COLOR_RGB2BGR)\n",
    "    mod_img_bgr = cv2.cvtColor(np.array(mod_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    gray_image = cv2.cvtColor(np.asarray(mod_img_bgr), cv2.COLOR_BGR2GRAY)\n",
    "    faces = hog_svm_detector(gray_image)\n",
    "    face_encodings_unknown = []\n",
    "    boxes_mod = []\n",
    "    for face in faces:\n",
    "        box = [face.left(), face.top(), face.width(), face.height()]\n",
    "        face_encodings_unknown.append(np.array(calculate_face_encoding(np.asarray(mod_img_bgr), box)))\n",
    "        boxes_mod.append((face.left(), face.top(), face.width(), face.height()))\n",
    "\n",
    "    face_encodings_orig = []\n",
    "    boxes_orig = []\n",
    "    for box, _, _, face_encoding_orig in orig_keypoints:\n",
    "        face_encodings_orig.append(np.array(face_encoding_orig))\n",
    "        boxes_orig.append(box)\n",
    "\n",
    "    count_of_matches = 0\n",
    "\n",
    "    for j, face_encoding_unknown in enumerate(face_encodings_unknown):\n",
    "        matches = face_recognition.compare_faces(face_encodings_orig, face_encoding_unknown, tolerance=0.55)\n",
    "\n",
    "        for i, match in enumerate(matches):\n",
    "            if match:\n",
    "                count_of_matches += 1\n",
    "                selected_color = palette[(j * 2) % num_colors]\n",
    "                bgr_color = tuple(int(value * 255) for value in selected_color)\n",
    "                box_orig = boxes_orig[i]\n",
    "                box_mod = boxes_mod[j]\n",
    "                top, right, bottom, left = box_orig[1], box_orig[0] + box_orig[2], box_orig[1] + box_orig[3], box_orig[\n",
    "                    0]\n",
    "                cv2.rectangle(orig_img_bgr, (left, top), (right, bottom), bgr_color, 6)\n",
    "\n",
    "                top, right, bottom, left = box_mod[1], box_mod[0] + box_mod[2], box_mod[1] + box_mod[3], box_mod[0]\n",
    "                cv2.rectangle(mod_img_bgr, (left, top), (right, bottom), bgr_color, 6)\n",
    "\n",
    "    orig_img_rgb = cv2.cvtColor(orig_img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    mod_img_rgb = cv2.cvtColor(mod_img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    return Image.fromarray(orig_img_rgb), Image.fromarray(mod_img_rgb), count_of_matches Code"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T15:46:49.422002Z",
     "start_time": "2024-02-05T15:46:49.398004300Z"
    }
   },
   "id": "289054ec09491e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This implementation takes an original image with the calculated keypoints, which also contains the 128D vector of the characteristics of the face and a potential modified image that is compared to the original image. The modified image is pre-processed and then the face bounding boxes are calculated using the svm+hog face detection algorithm. The bounding boxes are then used to compute the 128D vector of the faces found in the image. \n",
    "\n",
    "Each face found in the modified image is then compared to each face in the original image. If the Euclidean distance is less than the given tolerance of 0.55, a match is registered. For each match, a pair of colored boxes is drawn in both of the images where the matching faces are. The number of pairs is also counted.\n",
    "\n",
    "Here is an example output of the algorithm:\n",
    "\n",
    "![Example of a recognized faces](images/detected-faces-examples/example_face_recognition.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e402ba0cabf69b2a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
